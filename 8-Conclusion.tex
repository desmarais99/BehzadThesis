\Chapter{Conclusion and future work}\label{sec:Conclusion}

In this thesis, the performance of seven student skills assessment models is assessed and used to define a framework for evaluating mode fit.  Model fit of a data set is defined as the similarity between the \textit{performance vector} obtained over this data set and, the \textit{performance prototype} vectors obtained over synthetic data sets.

Let us return to the conjecture that the comparison of performance vectors over synthetic data can help determine whether a specific skill model corresponds to the ground truth of some data set. As described, the standard practice is to select the model that has the highest predictive performance as the best fit.  Using synthetic data, we have shown that the generative model does not always correspond to the best performer, and that an approach based on defining a vector space model of performance and on finding the nearest synthetic data point to the target data in that space provides a more reliable means to find the underlying model behind the target data.

This means of determining a data set's ground truth is made possible because the synthetic data sets have very distinct performance patterns, showing sharp differences across models.  The data sets from a single model tend to share a strong similarity among themselves, and show strong dissimilarity from most other models.  This property of synthetic \textit{performance vectors} cluster around distinct \textit{performance prototypes} is very strong for synthetic data.

Another finding is that we find evidence that data sets that share a common source have correlated \textit{performance vectors}.  This happens with the Fraction data sets.  They all have a similar pattern of performances across different subsets of items, different number of skills (latent factors), and different variants of the models as expressed by variations in the Q-matrices. Only when the Q-matrix has a very different property, namely a single skill per item, we do observe a different \textit{performance vector} for the models that depend on the Q-matrix (NMF conjunctive, DINA, NMF additive, and DINO).  

However, the similarity of \textit{performance vectors} does not seem to substantially extend to data that shares the same domain: although the Vomlel data is within the same domain as the Fraction data, namely arithmetic, the performance signatures are relatively different between the two.  This could be attributed to the Q-matrices involved (although both are multiple skills and therefore the difference is not due to the formal property of single skill per item), but other factors could also be involved such as sensitivity to data specific parameters.

The other interesting finding is that for real data sets the performances are not better than the expected performance and the conclusion behind that can be either the ground truth is not among the candidate models, or the best performer is not necessarily the ground truth. 



The highest correlation was for Vomlel dataset, which selects IRT as the ground truth. As expected, for the ECPE data set, the \textit{performance vector} is close to that of random data which resulted in very small differences among model performances. Note that this dataset comes from English examination domain and our investigation shows that a simple query among records retrieves all possible combination of test results, which is an evidence of randomness.

The results of assessing model fit for synthetic data show that some datasets with different ground truths that share some concepts, show a high correlation. For example, datasets based on linear conjunctive and DINA model. There exist no correlation for those that have completely different underlying models.

Furthermore investigations show that the predictive performance of each model over a dataset is dependent to different model and data specific parameters. For some parameters the \textit{performance vector} stays stable but some others play an important role in changing this vector. Although the predictive \textit{performance vector} changes for some parameters but it still shows a high correlation with datasets with the same ground truth.


The most important finding of this research is that the best performer may not be the model that is most representative of the ground truth, but instead it may be the result of contextual factors that make this model outperform the ground truth one. The last experiment tests the accuracy of signature approach versus best performer to classify a set of synthetic datasets with different contextual parameters. The results confirm that the ``signature'' approach shows a better accuracy in terms of measures for the confusion table in the span of different parameter conditions. This also shows the effect of data generation parameters on the best performer and reliability of these approaches on different assumptions about the data.

\section{Limits}

  \begin{enumerate}
  \item This study limits to static data opposed to dynamic data. Dynamic skill assessment models such as BKT are very important in EDM.  
  \item This research is also limited mostly to fraction data where students skill assessment in other domains has the same importance.%limit on generalization
  \item Most of our findings were through experiments over synthetic data. Although synthetic data is widely used to validate research results but it does not have the complexity of the real world.%: generality of the findings
  \item We test this approach with 7 models but still the number of candidate models and their complexity could be an important factor. %dependent on the models; would it work if we only had 2 models?
  \end{enumerate}

\section{Future Work}

Further studies with real and simulated data are clearly needed.  For example, there are other real datasets that potentially require a different set of candidate models. And this requires new approaches to generate synthetic data with respect to these skills assessment models.

However, the approach would generalize to dynamic data as well. This approach is using static models to assess a model fit but dynamic models are also used widely in EDM. This contribution also requires data generation and performance assessment of each model.

In this thesis we compared the ``signature'' approach with the ``best performer'' in terms of ground truth classification on synthetic data. The potential future contribution would be preparing a survey on different model selection approaches with their error metrics such as likelihood and comparing their performance over synthetic data.

One important future work is to give different weights to different parts of the space to justify the performance gain where in our research we assumed that the space is equally important.

Finally, because selecting right model is an important factor in all fields of studies, this approach could be applied in general fields as well. EDM is the example that we applied in our research but it can be tested in other fields of studies.
