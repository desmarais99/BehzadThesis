\Chapter{Conclusion and future work}\label{sec:Conclusion}

In this thesis, the performance of seven student skills assessment models is assessed and used to define a framework for evaluating mode fit. Model fit of a data set is defined as the similarity between the \textit{performance vector} obtained over this dataset and, the \textit{performance prototype} vectors obtained over synthetic datasets.

Let us return to the conjecture that the comparison of performance vectors over synthetic data can help determine whether a specific skill model corresponds to the ground truth of some dataset. As described, the standard practice is to select the model that has the highest predictive performance as the best fit. Using synthetic data, we have shown that the generative model does not always correspond to the best performer, and that an approach based on defining a vector space model of performance and on finding the nearest synthetic data point to the target data in that space provides a more reliable means to find the underlying model behind the target data.

This means of determining a dataset's ground truth is made possible because the synthetic datasets have very distinct performance patterns, showing sharp differences across models. The datasets from a single model tend to share a strong similarity among themselves, and show strong dissimilarity from most other models. This property of synthetic \textit{performance vectors} cluster around distinct \textit{performance prototypes} is very strong for synthetic data.

Another finding is that datasets which share a common source have correlated \textit{performance vectors}. This happens with the Fraction datasets. They all have a similar pattern of performances across different subsets of items, different number of skills (latent factors), and different variants of the models as expressed by variations in the Q-matrices. Only when the Q-matrix has a very different property, namely a single skill per item, we do observe a different \textit{performance vector} for the models that depend on the Q-matrix (NMF conjunctive, DINA, NMF additive, and DINO). 

However, the similarity of \textit{performance vectors} does not seem to substantially extend to data that shares the same domain: although the Vomlel data is within the same domain as the Fraction data, namely arithmetic, the performance signatures are relatively different between the two. This could be attributed to the Q-matrices involved (although both are multiple skills and therefore the difference is not due to the formal property of single skill per item), but other factors could also be involved such as sensitivity to data specific parameters.

The other interesting finding is that for real datasets the performances are not better than the expected performance and the conclusion behind that can be either the ground truth is not among the candidate models, or the best performer is not necessarily the ground truth. 



The highest correlation was for Vomlel dataset, which selects IRT as the ground truth. As expected, for the ECPE dataset, the \textit{performance vector} is close to that of random data which resulted in very small differences among model performances. Note that this dataset comes from English examination domain and our investigation shows that a simple query among records retrieves all possible combination of test results, which is an evidence of randomness.

The results of assessing model fit for synthetic data show that some datasets with different ground truths that share some concepts, show a high correlation. For example, datasets based on linear conjunctive and DINA model. There exists no correlation for those that have completely different underlying models.

Furthermore investigations show that the predictive performance of each model over a dataset is dependent to different model and data specific parameters. For some parameters the \textit{performance vector} stays stable but some others play an important role in changing this vector. Although the predictive \textit{performance vector} changes for some parameters but it still shows a high correlation with datasets with the same ground truth.


The most important finding of this research is that the best performer may not be the model that is most representative of the ground truth, but instead it may be the result of contextual factors that make this model outperform the ground truth one. The last experiment tests the accuracy of signature approach versus best performer to classify a set of synthetic datasets with different contextual parameters. The results confirm that the ``signature'' approach shows a better accuracy in terms of measures for the confusion table in the span of different parameter conditions. This also shows the effect of data generation parameters on the best performer and reliability of these approaches on different assumptions about the data.

\section{Limitations}

In this thesis we have proposed a framework for model selection on static data where we assume the test data represents a snapshot in time. This study limits to static data opposed to dynamic data. Dynamic skill assessment models such as BKT are very important in EDM and recently \citet{Rosenberg2015} proposed an approach for model selection on dynamic data. 

This research also has limitations on generality of the findings. The results of this thesis on real data are limited mostly to fraction data where student's skill assessment in other domains has the same importance. Moreover, most of our findings were through experiments over synthetic data. Although synthetic data is widely used to validate research results but it does not have the complexity of the real world. 

Furthermore, the approach that we have proposed is limited to skills assessment models that are described in chapter \ref{sec:RevLitt}. Despite the fact that recent improvements on model selection proposed frameworks with single skills assessment model, \textit{signature} approach considers 7 models. Still the number of candidate models and their complexity could be an important factor that requires further investigations.

\section{Future Work}

Further studies with real and simulated data are clearly needed. For example, there are other real datasets that potentially require a different set of candidate models. And this requires new approaches to generate synthetic data with respect to these skills assessment models.

However, the approach would generalize to dynamic data as well. This approach is using static models to assess a model fit but dynamic models are also used widely in EDM. This contribution also requires data generation and performance assessment of each model.

The experiments in this were about model selection at the model level in the performance space oppose to \citet{Rosenberg2015}'s work which is at parameter level. The other future work is to apply this method to identify the ground truth at the parameter level for an individual model algorithm.

In this thesis we compared the ``signature'' approach with the ``best performer'' in terms of ground truth classification on synthetic data. The potential future contribution would be preparing a survey on different model selection approaches with their error metrics such as likelihood and comparing their performance over synthetic data.

One important future work is to give different weights to different parts of the space to justify the performance gain where in our research we assumed that the space is equally important.

Finally, because selecting right model is an important factor in all fields of studies, this approach could be applied in general fields as well. EDM is the example that we applied in our research but it can be tested in other fields of studies.
