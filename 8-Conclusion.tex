\Chapter{Conclusion and future work}\label{sec:Conclusion}

\section{Conclusion}
In this thesis, we introduced seven student skills assessment models with different characteristics to be used to assess model fit. The main contribution is to assess model fit of a student model with comparison of the model performances over synthetic and real data. It is a complex task since there are many parameters involved in synthetic data generation process that can potentially affect the results of model selection. In this chapter, we summarize the results, conclusions, and possible future works.


% summery of main findings  and cuntribution

Let us return to the conjecture that the comparison of real vs.\ synthetic data can help determine whether a specific skill model corresponds to the ground truth of some data set. As described, the standard practice is to select the model that has the highest predictive performance but since there are different parameters involved in obtaining the performance, the actual best fitting may have been overlooked. The contribution of this thesis is to design an approach to recover such a situation. The proposed approach uses synthetic data to assess the goodness of different model fits by defining a vector space based on model performances. This is a complex question but some hints are given in the results.  

A clear finding is that the synthetic data sets have very distinct performance patterns, showing sharp differences across models.  In that respect, synthetic data do have a distinct patterns or specifically distinct positions in performance space. And none of the real data sets display the sharp differences found in all but one of the data sets which is Vomlel data.  

The Fraction data sets do display a similar pattern of performances across different subsets of items, different number of skills (latent factors), and different variants of the models as expressed by variations in the Q-matrices. Only when the Q-matrix has the property of a single skill per item do we observe a very different performance vector for the models that depend on the Q-matrix (NMF conjunctive, DINA, NMF additive, and DINO).  The other models are not affected (expected, POKS and IRT).  Therefore, in the domain of skills modeling, we find evidence that data from a common source does have correlated performance vectors as long as the models do not have large formal differences.

The other interesting finding is that for real data sets the performances are not better that expected performance and the conclusion behind that can be either the ground truth is not among the candidate models or the best performer is not necessarily the ground truth. Considering Best performer as a model selection approach then the challenge appears when none of the performances are better than the expected value.

That said, even though the Vomlel data is within the same domain as the Fraction data, namely arithmetic, the performance signature of the Q-matrix dependent models display two very different patterns.  This could be attributed to the Q-matrices involved (although both are multiple skills and therefore the difference is not due to the formal property of single skill per item), but other factors could also be involved such as sensitivity to other variables.

Therefore, we do find evidence to support the claim that the relative performance of the different skills modeling approaches create signatures over data sets.  And these signatures carry the potential to provide evidence about the ground truth. The other perspective is to consider the performances in a space where a dataset has a position in this performance space. In our research we used the nearest neighbor as a measure of model selection.

Comparing the performance vectors of two datasets with the same underlying models shows a high correlation while this measure is lower for those with different ground truth. This result was shown on synthetic data and also we calculated the same measure of goodness of fit for real data to assess a model fit. The highest correlation was for Vomlel dataset which selects IRT as the ground truth. As expected from the performance vector of ECPE data which was almost a flat signature, this data shows a high correlation with random data. The reason for this conclusion is that ECPE is a big dataset for English examination and a simple query can retrieve all possible combination of results which is an evidence of randomness.

The results of assessing model fit for synthetic data show that some datasets with similar underlying models that represent almost the same concept show a high correlation namely datasets based on linear conjunctive and DINA model.For two different synthetic datasets but with the same underlying model there is a high correlation. There exist no correlation for those that have completely different underlying models. In general for synthetic data assessing model fit is almost close to 100\%. 

Furthermore investigations show that the predictive performance of each model over a dataset is dependent to different model and data specific parameters. For some parameters the performance vector stays stable but some others play an important role in changing this vector. Results indicate that in the cases that the predicitve performance vector is changing the pattern of the signature stays stable. 

%Also we discuss the stability of the model performance vector in the space and its uniqueness for different data generation parameters such as sample size, average success  rate, number of skills, number of items, examinee and item variance. Results of this experiment show that the performance vector is sensitive to some data generation parameters. Some parameters like number of skills has a tangible effect on model performance pattern unlike some others such as sample size. Generally the pattern slightly changes through predictive performances. 
%The other findings of this research is the stability of the signature over different data generation parameters. The result of changing these parameters show that the pattern stays still over different parameters for datasets with same skills assessment model. Some variables such as sample size do not have tangible effect on the signature but some are shifting the signature upward or downward. In general the pattern stays almost correlated to the basic signature for a model. To make a comprehensive comparison we introduced a measure of similarity which is finding the nearest neighbor in the space of performances based on correlation. The result shows that the those dataset (among datasets with different set of data generation parameters) are very highly correlated and those that share basic concepts such as DINA and NMF conjunctive also shows good correlation over their performances since they share a conjunctive model of Q-matrix in their model. Therefore, a dataset can be described with a model and a set of contextual parameters.
% Our results on the accuracy of two methods on the classification of dataset to their underlying model shows that on different condition of data generation parameters only $73\%$ of the cases are correctly classified based on the best performer model. Whiles considering the nearest neighbor signature on the basis of correlation results in $90\%$ of correct classification. 


Testing ``signature'' approach on different data specific parameters affected the results. And it also affects the results of ``best performer'' approach. The most important finding of this research is that the best performer is not necessarily the best fit for a dataset. Results of the last experiment which was testing these two classification methods on synthetic datsets with different model and data specific parameters show that the ``signature'' approach shows a better accuracy in terms of measures for the confusion table in the span of different parameter conditions. This also shows the effect of data generation parameters on the best performer and also the stability of the signature on different sets of data generation parameters.
%%%%%%%%%%%%

%limits of my work and other works
\subsection{Limits}


To summarize the difference between our experiment and \cite{Desmarais2010} we can say that our work is a kind of extension to this research. Both of them are comparing the behavior of different datasets with different underlying structure. The difference is in the number of predictive performance models. \cite{Desmarais2010} uses only one technique to fit a model but in our work we compare a set of models which create a signature and those datasets that have similar signature can reflect similar characteristics. In this sense any skills assessment model can be added to the list of candidate models to make the prediction authentic.

%%%%HERE%

%Tensor based approach(Does it apply to time models)
%Can we apply it in general fields of studies
%

%


%%%%%%%%%%%%





\section{Future Work}
 % However, the approach would generalize to dynamic data as well.
 %survey on different error metrics
 %survey on different approaches
 

%Further studies with real and simulated data are clearly needed.  For example, we would like to know what is the mapping accuracy degradation when an incorrect number of skills are modelled.  And, naturally, a study with real data is necessary to establish if the approach is reliable in practice.

%The other contribution on this field is to consider different items with different models in a test result. Also there are different methods to generate the synthetic datasets which could effect the signature. In general we can extend this method to other fields of study. Defining some predictors and testing the predictive performance of a dataset with different variables can result this signature which can assess a model fit.
