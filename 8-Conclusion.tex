\Chapter{Conclusion and future work}\label{sec:Conclusion}

\section{Conclusion}
In this thesis, we introduced seven student skills assessment models with different characteristics to be used to assess model fit. The main contribution is to assess model fit of a student model with comparison of the model performances over synthetic and real data. It is a complex task since there are many parameters involved in synthetic data generation process that can potentially affect the results of model selection. In this chapter, we summarize the results, conclusions, and possible future works.


% summery of main findings  and contribution

Let us return to the conjecture that the comparison of real vs.\ synthetic data can help determine whether a specific skill model corresponds to the ground truth of some data set. As described, the standard practice is to select elect the model that has the highest predictive performance as the best fit.  However, using synthetic data, we have shown that the generative model does not always correspond to the best performer, and that an approach based on defining a vector space model of performance and on finding the nearest synthetic data point to the target data in that space provides a more reliable means to find the underlying model behind the target data.

Another finding is that the synthetic data sets have very distinct performance patterns, showing sharp differences across models.  In that respect, synthetic data have distinct positions in the performance space. However, the real data sets tend to be centered around the Expected value model.  They are also more similar to one another than the synthetic data sets.

The Fraction data sets do display a similar pattern of performances across different subsets of items, different number of skills (latent factors), and different variants of the models as expressed by variations in the Q-matrices. Only when the Q-matrix has the property of a single skill per item do we observe a very different performance vector for the models that depend on the Q-matrix (NMF conjunctive, DINA, NMF additive, and DINO).  The other models are not affected (expected, POKS and IRT).  Therefore, in the domain of skills modeling, we find evidence that data from a common source does have correlated performance vectors as long as the models do not have large formal differences.

The other interesting finding is that for real data sets the performances are not better that expected performance and the conclusion behind that can be either the ground truth is not among the candidate models or the best performer is not necessarily the ground truth. Considering Best performer as a model selection approach then the challenge appears when none of the performances are better than the expected value.

That said, even though the Vomlel data is within the same domain as the Fraction data, namely arithmetic, the performance signature of the Q-matrix dependent models display two very different patterns.  This could be attributed to the Q-matrices involved (although both are multiple skills and therefore the difference is not due to the formal property of single skill per item), but other factors could also be involved such as sensitivity to other variables.

Therefore, we do find evidence to support the claim that the relative performance of the different skills modeling approaches create signatures over data sets.  And these signatures carry the potential to provide evidence about the ground truth. The other perspective is to consider the performances in a space where a dataset has a position in this performance space. In our research we used the nearest neighbor as a measure of model selection.

Comparing the performance vectors of two datasets with the same underlying models shows a high correlation while this measure is lower for those with different ground truth. This result was shown on synthetic data and also we calculated the same measure of goodness of fit for real data to assess a model fit. The highest correlation was for Vomlel dataset which selects IRT as the ground truth. As expected from the performance vector of ECPE data which was almost a flat signature, this data shows a high correlation with random data. The reason for this conclusion is that ECPE is a big dataset for English examination and a simple query can retrieve all possible combination of results which is an evidence of randomness.

The results of assessing model fit for synthetic data show that some datasets with similar underlying models that represent almost the same concept show a high correlation namely datasets based on linear conjunctive and DINA model.For two different synthetic datasets but with the same underlying model there is a high correlation. There exist no correlation for those that have completely different underlying models. In general for synthetic data assessing model fit is almost close to 100\%. 

Furthermore investigations show that the predictive performance of each model over a dataset is dependent to different model and data specific parameters. For some parameters the performance vector stays stable but some others play an important role in changing this vector. Results indicate that in the cases that the predicitve performance vector is changing the pattern of the signature stays stable. 

%Also we discuss the stability of the model performance vector in the space and its uniqueness for different data generation parameters such as sample size, average success  rate, number of skills, number of items, examinee and item variance. Results of this experiment show that the performance vector is sensitive to some data generation parameters. Some parameters like number of skills has a tangible effect on model performance pattern unlike some others such as sample size. Generally the pattern slightly changes through predictive performances. 
%The other findings of this research is the stability of the signature over different data generation parameters. The result of changing these parameters show that the pattern stays still over different parameters for datasets with same skills assessment model. Some variables such as sample size do not have tangible effect on the signature but some are shifting the signature upward or downward. In general the pattern stays almost correlated to the basic signature for a model. To make a comprehensive comparison we introduced a measure of similarity which is finding the nearest neighbor in the space of performances based on correlation. The result shows that the those dataset (among datasets with different set of data generation parameters) are very highly correlated and those that share basic concepts such as DINA and NMF conjunctive also shows good correlation over their performances since they share a conjunctive model of Q-matrix in their model. Therefore, a dataset can be described with a model and a set of contextual parameters.
% Our results on the accuracy of two methods on the classification of dataset to their underlying model shows that on different condition of data generation parameters only $73\%$ of the cases are correctly classified based on the best performer model. Whiles considering the nearest neighbor signature on the basis of correlation results in $90\%$ of correct classification. 


Testing ``signature'' approach on different data specific parameters affected the results. And it also affects the results of ``best performer'' approach. The most important finding of this research is that the best performer is not necessarily the best fit for a dataset. Results of the last experiment which was testing these two classification methods on synthetic datsets with different model and data specific parameters show that the ``signature'' approach shows a better accuracy in terms of measures for the confusion table in the span of different parameter conditions. This also shows the effect of data generation parameters on the best performer and also the stability of the signature on different sets of data generation parameters.
%%%%%%%%%%%%

%limits of my work and other works
\subsection{Limits}


To summarize the difference between our experiment and \cite{Desmarais2010} we can say that our work is a kind of extension to this research. Both of them are comparing the behavior of different datasets with different underlying structure. The difference is in the number of predictive performance models. \cite{Desmarais2010} uses only one technique to fit a model but in our work we compare a set of models which create a signature and those datasets that have similar signature can reflect similar characteristics. In this sense any skills assessment model can be added to the list of candidate models to make the prediction authentic.

Although \citet{Rosenberg2015} work is the only reference in for model selection in time based skills assessment models but their work is limited to only BKT model which means the results to determine a dataset with a ground truth other than BKT should be evaluated. The limit of our work in the comparison with \citet{Rosenberg2015} is that the models are statistic which could be improved to adapt dynamic models.
%%%%HERE%

\section{Future Work}

Further studies with real and simulated data are clearly needed.  For example, there are other real datasets that potentially require a different set of candidate models. This means new approaches to generate synthetic data with respect to these skills assessment models.

%Tensor based approach(Does it apply to time models)

However, the approach would generalize to dynamic data as well. This approach is using static models to assess a model fit but dynamic models are also used widely in EDM. This contribution also requires data generation and performance assessment of each model.

 %survey on different error metrics
 %survey on different approaches
In this thesis we compared the ``signature'' approach with the ``best performer'' in terms of ground truth classification on synthetic data. The potential future contribution would be preparing a survey on different model selection approaches with their error metrics and comparing their performance over synthetic data.

%Can we apply it in general fields of studies 
Finally, because selecting right model is an important factor in all fields of studies, this approach could be applied in general fields as well. EDM is the example that we applied in our research but it can be tested in other fields of studies.
