\Chapter{SYNTHETIC DATA GENERATION}\label{sec:Syn}

The framework of this study relies on synthetic data. The performance signature vectors are based on synthetic data generated from each model of the performance space.  Therefore, we dedicate a full chapter to the process of generating synthetic data from each skills assessment models described in chapter \ref{sec:RevLitt}.

A unique advantage of synthetic data is that the model parameters can be predefined. Once the simulated data is generated with predefined parameters, a model can be trained over the generated data and a comparison with the original, known parameter values becomes possible. This may be the strongest benefit of synthetic data in assessing model fit. Since the hypothesis of this research relies on comparison the performance vector of real vs. synthetic data, then we can also consider data specific parameters of the real data in the generation process to make a better comparison of the results.

\section{Data generation parameters}

As described before, there exists two types of parameters for student test outcome:
\begin{itemize}
\item Model specific: Models have their own parameters, some of which may be shared among models, such as a Q-matrix for multiple skills models, some of which are more unique such as the item discrimination parameter of the IRT class of models. Generally an experiment should be designed to learn these parameters form a dataset or they can be generated under specific criteria. More details for each parameter is given later in this chapter.
\item Data specific (contextual): The data specific parameters are common to all data sets.  They have to be predefined, but they may be set to reflect a data set for which we want to find the ground thruth.  For example, the item difficulty distribution or the student success rate distribution are two data specific parameters.  Data specific parameters can also be a model parameter, such as item difficulty and discrimination.
\end{itemize}

Table \ref{tbl:param-DataGen} shows a complete list of both types of parameters required for data generation.

\begin{table}
  \centering

\begin{tabular}{c|c|l!{\VRule[1.5pt]}l|l!{\VRule[1.5pt]}l|}
\multicolumn{3}{c}{}&\multicolumn{3}{c}{Parameters}\tabularnewline
\cline{4-6}
\multicolumn{3}{c!{\VRule[1.5pt]}}{Skills Model}&\multicolumn{2}{c!{\VRule[1.5pt]}}{Model specific}&\multicolumn{1}{c|}{Data specific}\tabularnewline
\cmidrule[1.5pt]{2-6}
&&NMF Conj. & & &  \tabularnewline
\cline{3-3}
&&NMF Add.&&\multirow{-2}{*}{ \tabitem  Q-matrix }&\multirow{-2}{*}{\tabitem Number of students } \tabularnewline
\cline{3-4}
&&DINA& \tabitem Slip & &\tabularnewline
\cline{3-3} 
&\multirow{-4}{*}{\begin{sideways} \scriptsize Multiple\end{sideways}}&DINO& \tabitem Guess&\multirow{-2}{*}{  \parbox[t]{3cm}{ \tabitem Students skills  \\ mastery matrix}}& \multirow{-2}{*}{\tabitem Number of items } \tabularnewline
\cmidrule[1.5pt]{2-5}
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Student ability  } &  \tabularnewline
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Item difficulty  } &\multirow{-2}{*}{\tabitem Number of skills } \tabularnewline
&&\multirow{-3}{*}{ IRT}&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Item discrimination}  &\tabularnewline
\cline{3-5}
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Student Odds  } & \multirow{-2}{*}{\tabitem Test success rate }  \tabularnewline
&\multirow{-5}{*}{\begin{sideways} \scriptsize Single \end{sideways}}&\multirow{-2}{*}{ Expected}&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Item Odds}  &  \tabularnewline
\cmidrule[1.5pt]{2-5}
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Initial Odds } & \multirow{-2}{*}{\tabitem Student score Variance} \tabularnewline
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{ \tabitem Odds ratio } & \tabularnewline
\multirow{-9}{*}{\begin{sideways} \scriptsize Contributed skills \end{sideways}}&\multirow{-3}{*}{\begin{sideways} \scriptsize Zero\end{sideways}}&\multirow{-3}{*}{ POKS }&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem \parbox[t]{.4\textwidth}{\raggedright Partial Order Knowledge Structure (KS) (includes two alpha error parameters for its induction from data)}}& \multirow{-2}{*}{\tabitem Item score Variance} \tabularnewline
\cmidrule[1.5pt]{2-6}
\end{tabular}
  \caption{Parameters involved in synthetic data generation}
  \label{tbl:param-DataGen}
  \note{Item difficulty and other ``model specific'' parameter can be obtained from data.  We may have to refine this taxonomy, but let's leave it as is for now.}
\end{table}

\subsection{Assessing parameters}

Both types of parameters are required to generate synthetic dataset. There are few steps to obtain these parameters:
\begin{itemize}
\item \textbf{Parameters estimated form real data}: In general, models define means to estimate their parameters from data.  Some are trivial to estimate, such as student success rate distribution if we assume a Gaussian distribution, for eg., and some are much more complex, which is the case of latent factors such as the number of skills and the Q-matrix itself.
\item \textbf{Initialized from a random distribution}: In the case that there exists no reference dataset to derive parameters we can randomly select some samples based on a distribution that can be estimated form the data. Some parameters have specific conditions that if they become violated it changes the definition of the parameter. More details is given in next sections.
\item \textbf{Expert given and arbitrary parameters}: Latent factors are often not derived from data but arbitrarily defined by expert or by some educated choice. The Q-matrix is the most obvious of such parameter, but some are the guess and slip parameters which can prove difficult to correctly estimate from data.

\end{itemize}

These steps are not essentially separated. Each of them is an alternative to provide parameters in the absence of others or a combination of them can be used to obtain a more complex parameter. 

The rest of this chapter describes the data generation process based on each skills assessment technique in chapter \ref{sec:RevLitt}.

\section{POKS}

As mentioned before this technique does not directly model latent skills and the inference relies on observable items only. 

\subsection{Obtaining parameters}
This model has three parameters with which the inference in possible: Partial order knowledge structure (let us name it KS for shot), Initial probabilities, Odds ratio. To extract these parameters from an existing data we use the POKS \footnote{http://www.professeurs.polymtl.ca/michel.desmarais/Papers/UMAP2011/lib-poks.R} R package to learn all the parameters given a dataset. Sometimes these parameters are defined by an expert. In the absence of these two alternatives, these parameters should be generated randomly. Below we describe how these parameters are generated with different conditions and constraints.

\subsubsection{Partial Order Knowledge Structure (KS)}
This parameter shows the relation and the dependency among a set of items. In fact, it reflects the order of learning a set of items in the same problem domain. Assuming items as different ``nodes'' in the graphical structure, there is a kind of ``parent-child'' relation between some pair of nodes where an easier item can be the child of a harder item. In our study this relation is considered as a ``link'' between two nodes. This link is not necessarily exists between each pair. Only those items that show a relation in the learning process can have a pairwise link. 

A KS is a directed acyclic graph, with the exception that cycles in the form of symmetric relations are allowed. Also in the current version of POKS transitive relations between items are explicitly ignored. 

KS is a complex parameter which has two other parameters involved in itself:
\begin{itemize}
\item Number of independent graphs: Consider the graphical demonstration of a KS. Even in a set of items there could be different groups of nodes without any dependency between each group. Since there exists no link between groups then they create an independent graph in the knowledge structure. This parameter can also affect the total number of links in a structure where more independent groups results in less number of links in the structure given same density of links in each group. 
\item Total Number of links: This has a correlation with the previous parameter but still we can control the number of dependencies in a KS. This parameter has a direct effect on test success rate and item score variance. Fewer number of links will result in lower test success rate and item score variance. There are still some other parameters that could change the test success rate and item score variance. In this sense manipulating one parameter may affect others.
\end{itemize} 

In the current study, the two parameters, $\alpha_p$ and $\alpha_c$, are respectively set at~$0.85$ and~$0.10$. 
Other input parameters can make the random generation more specific, such as the number of links and number of independent trees in a KS. These parameters will change the item variance in the test result matrix. 

Adjacency matrix  is one way to represent the KS. To avoid having a cycle in the structure we randomly assign $0$~-~$1$ values to an upper triangular of this matrix. For simplicity we consider the structure to be a single connected graph and the number of links corresponds to half of the cells of the upper triangular of this matrix. figure \ref{fig:KSExample} shows a adjacency matrix associated with its graphical structure. 


\begin{figure}[h]
\centering 
\subfigure[Adjacency matrix ]{  $\begin{array}{ccc}
&\begin{array}{ccccc}
i_{1} & i_{2} & i_{3} & i_{4}& i_{5}\end{array}&\\
\begin{array}{c}
i_{1}\\
i_{2}\\
i_{3}\\
i_{4}\\
i_{5}
\end{array}&\left[\begin{array}{ccccc}
0 & 0 & 0 & 0 &1\\
0 & 0 & 1 & 0 &0\\
0 & 0 & 0 & 1 &1\\
0 & 0 & 0 & 0 &0\\
0 & 0 & 0 & 0 &0
\end{array}\right]&\\
&&\\
&&
\end{array}$}
\qquad
\qquad
\subfigure[Graphical representation ]{ $\begin{array}{cc}
\includegraphics[trim=7cm 20.4cm 11cm 3.5cm,width=.20\linewidth]{SampleKS} &
\end{array}$}

\caption{An example of random KS with 5 items}
\label{fig:KSExample}
\end{figure}


Once the KS is generated we should initialize the two other parameters : initial probability of all items and odds ratio for some pairs of items. One of the important concepts for this purpose is the ``level'' of a node in the graphical representation of the KS which corresponds to the longest path from that node to a leaf node. A leaf node is a node that does not have any child and a root is a node without any parents. Equation \ref{EQ:Level} shows the definition of this function where $length-of$ returns the length of a path between two nodes; and $path$ returns $true$ value if there exists a directed path between two nodes; and $leaf$ returns $true$ if the node is a leaf.  For example in figure \ref{fig:KSExample} $I_{5}$ is a leaf with the level of $1$ and $I_{2}$ is a root with level of $3$.

\begin{equation}
L(KS,N_{i}) = max(E|E=length-of(N_{i},p)  ,  \exists path(N_{i},p)  , leaf(p)=True)
\label{EQ:Level}
\end{equation}

\subsubsection{Initial Probabilities}

Each item should be associate with an initial probability of success. This probability has a direct correlation with the difficulty of the item. This is an example where a data specific parameter is also a model parameter. In general if node $X_a$ implies node $X_b$ ($X_a \rightarrow X_b$) then node $X_b$ should have a higher initial probability of success because in this relation item $X_a$ is the harder problem. The allocation of initial values to items should be done respect to this characteristic. 

Starting from a root node in KS we assign a random initial probability for each node $N_{i}$ from a range of values between $r_{max}$ to $r_{max}+\frac{1-r_{max}}{L(KS,N_{i})}$ where $r_{max}$ is the maximum initial probability of $N_{i}$'s parents. For a node that is a root $r_{max}$ is $0$. Obviously it should have the lowest initial probability among its descendant. Also for a leaf the assignment range will be between $r_{max}$ and $1$ because it has the least level. 


As mentioned before, the initial probability of success for items come from these values. To sample test outcome for a record(student) we start with initial probabilities. This set of values changes when a sample is assigned to each item. Let us call this variable success probability set as the \textbf{state} of probabilities (shown as $S$ where $S_i$ is the probability of success for item $i$). In this context we use odds instead of probabilities where $O(H) = \frac{P(H)}{1-P(H)}$. The sampling process for a single record preforms item by item and in each step $S$ vector gets updated. The details of updating the ``state of odds'' and assigning samples to items are given in next sections.

\subsubsection{Odds ratio}

The very last parameter is the odds ratio which is a ratio that represents the strength of a link between a pair of items. In the inference for POKS model this parameter is used to update the initial probability of inferred items given a set of observations. We also use this parameter to update the state of odds once an item has been sampled (more on these implementation details is given in the next section).
For inference in POKS, the probability update for node $H$ given $E_1$,... $E_n$ can be written in following posterior odds form:

\begin{equation}
O(H|E_1,E_2, ... , E_n) = O(H) \prod_{i}^{n} \frac{P(E_i|H)}{P(E_i | \overline{H})}
\label{EQPOKSratio}
\end{equation}
where odds definition is $O(H|E) = \frac{P(H|E)}{1-P(H|E)}$ and $O(H)$ is the initial odds of node $H$. If evidence $E_i$ is negative for observation $i$, then the ratio $\frac{P(\overline{E_i}|H)}{P(\overline{E_i}|\overline{H})}$ is used. We follow the same steps to generate data based on POKS model. 

There are two types of odds ratio in the POKS model. Consider $I_{3}$ in figure \ref{fig:KSExample} which has two children and a parent . If the sampling result for this item (with respect to its initial odds) becomes $1$ (success) then we update the odds of its children because a success for a harder item should increase the chances of success for an easier one. Therefore we define ``true odds ratio'' which is applicable when the evidence (in this case the evidence is the one that has been sampled,  node $I_{3}$) becomes a success. If the evidence is $0$ (failure) then we update the odds of its parents since a failure for an easier problem should decrease the chances to success a hard one. To update the parents' odds we use ``False odds ratio'' when the evidence is false. In fact $\frac{P(E_i|H)}{P(E_i | \overline{H})}$ in equation \ref{EQPOKSratio} represents the odds ratio of the item $H$ given evidence $E_i$. 

In our experiments, in cases of random parameter generation, we assign values greater than $0.5$ to ``true odds ratios'' and smaller than $0.5$ to ``false odds ratios'' for those pairs of items that have link in KS. 

\subsection{Data generation}

Previous sections explained the process of obtaining a random generated set of parameters for POKS. This is useful if the expert defined parameters or even the estimated parameters form real data are not available. In this section we explain the process for sampling data points values to create a synthetic student test result matrix given a set of parameters. Each record in this approach represent a student test result that requires $n$ iterations to be generated where $n$ is the number of items and the final result matrix needs $m$ (number of students) records. The process simply follows steps in algorithm \ref{POKS:ALG}. Line 8 to 15 of this algorithm updates the state odds of the items. It is important to note that we just look at the neighbors of the item that is sampled. In other words the update propagates through a breadth-first search.


\begin{algorithm}
\caption{POKS data generation}
\label{CHalgorithm}
\begin{algorithmic}[1]
\For{each record $i$ \Pisymbol{psy}{206} $m$ (number of students) }
\State or.f = False Odds Ratio
\State or.t = True Odds Ratio
\State  $S$ = Initial odds
\For{each item $j$ \Pisymbol{psy}{206} $n$ (number of items) }
\State Randomly pick item $j$ that has not been sampled
\State $R_{ij}$ =  Sample item $j$ for record $i$ with respect to $S_j$
\If{$R_{ij} = 1$}
\State $U =$ children of item $j$ in $KS$
\State  $\forall c\in U$:  $S_c = S_c\times or.t_{cj}$
\EndIf
\If{$R_{ij} = 0$}
\State $U =$ parents of item $j$ in $KS$
\State  $\forall p\in U$:  $S_p = S_p\times or.f_{pj}$
\EndIf
\EndFor
\EndFor
\end{algorithmic}
\label{POKS:ALG}
\end{algorithm}



\subsection{Data specific parameters}

In some experiments, we need to generate a synthetic data with specific mean success rate for both students and items, and with specific variance, or specific entropy.

One way to control the test result success rate and student/item score variance is to apply changes on the initial odds of items which are used to sample student test result. To control the student scores variance, one can scale the initial odds of each record such that the distribution of the initial odds stays the same for all students; for example we can double all the initial odds to represent a student with higher success rate but still the distribution of items score variance remains the same. Changing the initial odds that follows a specific distribution will create a dataset in which the item variance is following that distribution. It is important to note that this change should not violate any relation in the KS. For overall success rate, the initial odds can be scaled regardless of the student or item perspective, for example tripling all initial odds for all students will result in a higher success rate than the default values. As mentioned before these parameters are somehow connected. Changing one parameter may affect others.



\section{IRT}

The standard IRT models student response outcome with a single skill. But in addition to the skill mastery, IRT includes three other parameters. Akin to all models these parameters can be derived from real data or an expert can define them, it is also possible to initialize these three parameters with a distribution. The rest of this section describes the steps to generate these parameters and the synthetic data.

\subsection{Generating parameters randomly}
Equation~\ref{IRTEQ} shows the probability of a student with ability $\theta$ to succeed item $j$ which has the difficulty of $b_j$ and discrimination of $a_j$ based on IRT-2PL model. 

\newcommand\pN{\mathcal{N}}

\begin{equation}
P(X_j\!=\!1\;|\;\theta) = \frac{1}{1+e^{-a_j(\theta-b_j)}}
\label{IRTEQ}
\end{equation}

We assume the following distributions for this model parameters:

\begin{center}
$\theta \sim \pN(\mu, s_t)$

$a_j \sim \operatorname{Pois} \left({\lambda_a}\right)$

$b_j \sim \operatorname{Pois} \left({\lambda_b}\right)$
\end{center}


The student related parameter is $\theta$ and it follows a standard normal distribution with two parameters $\mu$ and $s_t$ which are respectively mean of student ability and individual examinee standard deviation. These two parameters can be obtained from an existing dataset or they can be assigned to specific values if desired. Two parameters $a$ and $b$ are item related parameters that are generated based on a poisson distribution with $\lambda_a$ and $\lambda_b$ to control item score variance.

Extremely large or small values for each of these parameters will result in an unrealistic outcome generation. Hence we bounded values for these parameters:

\begin{eqnarray*}
-4 < \theta_i < 4\\
0.5 < a_j < 3\\
-3 < b_j < 3
\end{eqnarray*}
where $i$ and $j$ are students and items respectively.

\subsection{IRT synthetic data process}

Once all parameters are prepared we can generate the simulated data with equation \ref{IRTEQ}. The results of \ref{IRTEQ} are probabilities between $0-1$ that should be discretized to $0$ or $1$ values. Test score success rate is the threshold to discrete the test result probabilities. Algorithm \ref{Alg:IRTGen} shows the steps to generate IRT based synthetic data. $\theta$ and $b$ are also a kind of data specific parameter which can control the item and student variance in the generated data.

\begin{algorithm}
\caption{IRT data generation}
\label{CHalgorithm}
\begin{algorithmic}[1]
\State $a = \operatorname{Pois} ({\lambda_a},$ Number of items $)$
\State $b = \operatorname{Pois} ({\lambda_b},$ Number of items $)$
\State $\theta = \pN(\mu, s_t ,$ Number of students$)$
\For{each record $i$ \Pisymbol{psy}{206} $m$ (number of students) }
\For{each item $j$ \Pisymbol{psy}{206} $n$ (number of items) }
\State $R_{ij} = (\frac{1}{1+e^{-a_j(\theta_i-b_j)}} >$ Test score successRate $)$
\EndFor
\EndFor
\end{algorithmic}
\label{Alg:IRTGen}
\end{algorithm}



\section{Linear models}
\label{LinearModelSynthetic}
Generating synthetic data with linear models is one way to cover multiple skills. In this study, Q-matrix is the only means to represent the mapping between skills and items. If the Q-matrix needs to be inferred from a data set, NMF is the technique we use in this study. Depending on the type of a Q-matrix, different models could be applied to NMF technique. In this section we describe how to generate synthetic datasets that reflect the characteristics of each model of the Q-matrices used for factorization technique. Section \ref{NMF_DESC} described all details to learn parameters of these models given a dataset. This section discusses the process of generating synthetic data given a set of parameters for these models.

The very first step to generate simulated test result for linear models is to define a Q-matrix that maps $k$ skills to $n$ items. The same as before these parameters can be inferred form an existing dataset but many existing datasets offer few expert defined Q-matrices. Some datasets even have more than a single expert defined Q-matrix with different number of skills. Section \ref{edm2014} introduced few methods to refine an expert-defined Q-matrix. A Q-matrix can also be generated randomly. As mentioned, there are three types of Q-matrices. In this study we use conjunctive and additive model in the factorization technique.The following sections describe generating parameters for each model.

\subsection{Q-matrix}
\label{Q-MatrixObtain}
Q-matrix is a required parameter for all linear models. Few mandatory parameters are required to create a random Q-matrix:

\subsubsection{Parameters}
\begin{itemize}
\item Number of items 
\item Number of skills: In some context there is a constraint for the number of latent skills which is: $k<nm/(n+m)$ \protect\citep{lee1999learning} where $k$ , $n$ and $m$ are number of skills, students and items respectively.
\item Maximum number of skills per item: This parameter can also reflect item difficulty level. The difficulty of  the two-skills items will further increase by the fact that they require the participation of their skills.
\item Item score variance: The only way to apply item score variance is to manipulate the Q-matrix. Harder items should require more skills than easier ones. Therefore the ratio of contributed skills to the total number of skills shows the variance of item scores which will be reflected in the final result matrix.
\end{itemize}



In the case of unavailable predefined Q-matrix, we defined a Q-matrix that provides all the possible combination of $k$ skills with a maximum of $Max$ skills per item, and at least one skill per item. A total of $\sum\limits_{k=1}^{Max} \dbinom{n}{k}$ candidate items span this space of combinations. For example $21$ items for $6$ skills and a maximum of $2$ skills per item. This matrix is shown in Figure~\ref{PerfectQ1}. Note that red cells represent $0$ or missing skills and yellow cells are $1$ or presence of skills. Items 1 to 15 are two-skills and items 16 to 21 are single-skill. All skills have same weight in a conjunctive type of Q-matrix. The Q-matrix can be created with randomly replicating or eliminating some candidate items to adjust the number of items to the desired number. Also item score variance can be controlled by picking some appropriate items to create high or low variance.


\subsubsection{Additive vs. Conjunctive}
The proposed approach to create conjunctive type of Q-matrix differs from additive type where a normalization process change the values of skills. The normalization process forces the weights that are assigned to skills to sum up to $1$ for each item.  Figure \ref{figNMFAddQM} shows an additive type of Q-matrix where skills have same weights for each item but different weights among items. 

\subsection{Skills mastery matrix (student profile)}
\label{Student_Profile}
Skills mastery or student profile is the other mandatory element for all linear models in this context, since the synthetic data is the product of the Q-matrix by the skills mastery matrix. Mapping skills to students also requires some mandatory parameters to create more specific synthetic test result matrix:
\subsubsection{Parameters}
\begin{itemize}
\item Number of students 
\item Number of skills: This number should match with the one in the Q-matrix. In this matrix skills show the ability of students to answer items.
\item Student score variance: In order to apply student score variance one should consider different abilities for students. The ability is, in fact, reflected by the set of skills that an individual is mastered. The assigned ability level will be reflected in the generated test result. Both Item and student score variance in our study are taken from a beta distribution:
\begin{eqnarray*}
\textrm{Student score} \sim Beta(\alpha_S, \beta_S)\\
\textrm{Item score} \sim Beta(\alpha_I, \beta_I)
\end{eqnarray*}

where $\alpha_S$, $\beta_S$, $\alpha_I$ and  $\beta_I$ are the shape parameters of the $Beta$ distribution of student and item score.

\end{itemize}

Students with fewer mastered skills should have low test score but in reality skills have different weights. For example examinees $E_1$ and $E_2$ are mastered in two unrelated skills $S_1$ and $S_2$ respectively. Although they both master single skill but they do not necessarily get same test scores. In this thesis we consider that skills have same level of difficulty which means that they have same weight in both Q-matrix for each item and skills mastery matrix for each student. 


\subsection{Synthetic data}

The process to create synthetic data based on additive type of Q-matrix is almost the same as Conjunctive one. The difference is on the interpretation of the Q-matrix that changes the step where the result matrix is producing. In the additive model a simple cross product of the Q-matrix and skills mastery will generate the test result matrix. For conjunctive a negation operator should be applied on both skills mastery and test result matrices: 
\begin{equation}
\begin{array}{c}
\text{Conjunctive model:   } \mathbf{R}=\neg\left(\mathbf{Q}\times\left(\neg\mathbf{S}\right)\right)\\
\text{Additive model:   } \mathbf{R}=\mathbf{Q}\times\mathbf{S}
\end{array}
\label{NMF_GEN_EQ}
\end{equation}

Figure~\ref{NMFAddNoisy} shows an artificial result matrix based on additive model of Q-matrix . Since the model is additive, there are some pale cells and the paler a cell is, the more chance a student has to succeed the question. In conjunctive model the result matrix is either $0$ or $1$. 

\note{Stopping here for now in this chapter.  2016.01.19.  There are many typos, grammatical errors, and ill structured sentences...}


\begin{figure}[ht]
\centering

\subfigure[Q-matrix of 6 skills]{
   \includegraphics[scale =0.5] {ExpectedQ.pdf}\label{PerfectQ1}
 }\quad
 \subfigure[Synthetic data of 100 students with 10\% slip and 20\% guess factor]{
   \includegraphics[scale =0.5] {ResultM.pdf}\label{Result_ofPerfectQ}
 }
\caption{Q-matrix and an example of simulated data with this matrix.  pale cells represent 1's and red ones represent 0's.}
\label{figqmatrixandResutM}
\end{figure}


\subsection{Noise factor}
\label{Noise_}
Finally, two more parameters are used in the simulated data, namely the $\mathit{slip}$ and $\mathit{guess}$ factors. They are essentially noise factors and there are three approaches to apply these factors to the result matrix:
\begin{itemize}
\item Student based: each student would have specific amount of these factors in his test result.
\item Item based: this represent how tricky a question can be. The greater they are for an item, the more tricky that item would be. This approach is used in DINA/DINO models (next section describes these models).
\item Overall: For synthetic data generated by linear models we use this type of noise. For example, with $s\%$ of slip and $g\%$ of guess factor ,this will result in approximately $s\%$ of the succeeded outcomes to become failed, and $g\%$ of the failed outcomes to become succeeded in the test result matrix.

\end{itemize}

Two samples of synthetic result matrices for conjunctive and additive model are given in figure~\ref{Result_ofPerfectQ} and  \ref{NMFAddNoisy} respectively where pale cells represent a value of 1 and red cells are 0. Examinee ability shows up as vertical patterns, whereas item difficulty creates horizontal patterns. As expected, the mean success rate of the 2-skills items 1 to 15 is lower than the single skill items 16 to 21. The same situation holds for additive model in figure \ref{figNMFAddgen}, the patterns in figure \ref{NMFAddNonNoisy} are more tangible where there are shades of colors that indicate different probability of success. Therefore, for a synthetic result matrix based on additive model, the values should be discretized with respect to an average success rate parameter. Figure \ref{NMFAddNoisy} shows a Discretized version of result matrix with 20\% slip and 10\% guess factor.


 \begin{figure}[h]
\begin{tabular}{c}
\subfigure[{Additive Q-matrix\label{figNMFAddQM}}]{\begin{footnotesize}
$\begin{array}{ccc}
 && \textrm{items}\\

\mathrm{\begin{sideways}skills\end{sideways}} & & \left[\begin{array}{cccccccccccccccccc}

0.00&0.00&0.25&0.00&0.00&0.00&0.33&0.33&0.5&0.0&0.0&0.5&0.0&0&0&0&0\\
0.25&0.00&0.25&0.25&0.00&0.00&0.00&0.33&0.0&0.0&0.0&0.0&0.0&0&1&0&0\\
0.25&0.25&0.25&0.25&0.00&0.33&0.00&0.33&0.0&0.5&0.0&0.0&0.0&1&0&0&0\\
0.25&0.25&0.00&0.00&0.33&0.33&0.00&0.00&0.5&0.0&0.5&0.0&0.0&0&0&0&0\\
0.25&0.25&0.00&0.25&0.33&0.33&0.33&0.00&0.0&0.0&0.0&0.0&0.5&0&0&1&0\\
0.00&0.25&0.25&0.25&0.33&0.00&0.33&0.00&0.0&0.5&0.5&0.5&0.5&0&0&0&1


\end{array}\right]
\end{array}$
\end{footnotesize}
}\\
\begin{tabular}{cc}
\subfigure[Raw Result matrix ]{
   \includegraphics[scale =0.5] {NMFAddNonNoisy.pdf}\label{NMFAddNonNoisy}
 }\quad
&
\subfigure[Discretized result matrix with 20\% slip and 10\% guess factor]{
   \includegraphics[scale =0.5] {NMFAddNoisy}\label{NMFAddNoisy}
 }\quad
\end{tabular}
\end{tabular}
\caption{Additive model of Q-matrix and Corresponding synthetic data}
\label{figNMFAddgen}
\end{figure}


\section{Cognitive Diagnosis Models}

From the family of cognitive diagnosis models we choose two models which are deterministic input noisy AND/OR. Detailed description of these models is given in section \ref{DINA-DINO-Desc}. This section discusses the approach that we took to generate synthetic datasets based on these models. The same as linear models, these models also represent the mapping of multiple skills to items and examinees in the form of a Q-matrix and skills mastery matrix respectively. To borrow the Q-matrix form an existing dataset we use NMF because these models can not infer a Q-matrix but they can estimate other model specific parameters such as student profile, slip and guess given a Q-matrix. 

To obtain a conjunctive type of Q-matrix for DINA model we follow the same approach as described in section \ref{Q-MatrixObtain} where the normalization process does not apply.  Generating the student profile is almost the same as section \ref{Student_Profile} but there are other parameters involved:


\subsection{Skill space}
Consider $K$ skills for a cognitive diagnosis model, there exists $2^K$ combination of skills to be considered for a student profile. This shows the maximum capacity of skill space. One of the parameters of student profile is skill class where only some candidates from this combinations are allowed to be presented in skills mastery matrix. This parameter has a benefit that can control the prerequisite skill in a student skills mastery vector given a partial order structure of skills.

\subsection{Skill distribution}

The skill class set is associated with a distribution which defines the probability of appearance of each combination indicated in skills class. i.e. If there exists 3 skills then the skill space will be 8 combinations and assuming skill class consists of 4 combinations out of these 8, then skill distribution  must be a vector of length 4, with sum equal to 1. Student score variance can also be set through this parameter. In this research, we use a normal distribution for this parameter.



\begin{center}
$Student$  $skills \sim \pN(\mu, s_s)$
\end{center}

A skill mastery matrix can be generated once these parameters are set.


\subsection{Slip and Guess}
one of the characteristics of DINA/DINO models is that they incorporate slip and guess factors. These parameters are essentially noise factors and in these models they are treated item-wised (section \ref{Noise_}). We defined a uniform distribution to cover a range of values for slip and guess:


\begin{center}
$Slip \sim \textit{U}(S_{lower-bound}, S_{upper-bound})$

$Guess \sim \textit{U}(G_{lower-bound}, G_{upper-bound})$
\end{center}

Once all the parameters are prepared we can generate an outcome of test $j$ for examinee $i$ from equation~\ref{DinoEQ3} where $\xi_{ij}$ is calculated based on AND or OR gates. Since these models are behaving based on a single value that represents student ability, we need to calculate an array of abilities for each item given a set of skills for an student. For DINO a disjunction is used between skills of an item and skills of a student to determine whether the student has the ability or not where for DINA a conjunction applies.

\begin{equation}
 R_{ij} \,=\, (1-s_j)^{\xi_{ij}} g_j^{1-\xi_{ij}}
\label{DinoEQ3}
\end{equation}

\section{Educational data generator}

Despite all the parameters and models that were described, it is still challenging to generate synthetic data for a specific model with a set of given parameters. We \citep{Trieu2015} created a package that generates synthetic data under a specific model's assumption given a set of parameters. As mentioned before these parameters can be fine grained which can be combined to create a more complex parameter. In this sense we can create a hierarchy for the complexity of these parameters. 

Previous sections described the generation process for each model. This task is easy and possible as long as two conditions are satisfied: 
\begin{itemize}
\item First: All the parameters for a model are given. Table \ref{tbl:param-DataGen} shows a list of required parameters to generate synthetic data.
\item Second: There exists no conflict between the given parameters. For example given ``number of skills'' as a simple parameter equal to $4$ and a predefined ``Q-matrix'' as a complex one with $5$ skills is a conflict to generate a synthetic data based on DINO model. However, testing these conflicts is not an easy task and also resolving a conflict is another challenge.
\end{itemize} 

For some of parameters we can use default values but some are mandatory such as number of skills. Therefore based on the hierarchical structure among parameters of a model we can define sets of sufficient parameters to generate data based on a model. Due to this structure there are different combination of parameters that can generate a synthetic data. For example, a synthetic dataset based on DINA can be generated with \{Q-matrix, Skills mastery matrix, Slip and Guess vectors\} or \{Number of skills, Items and students\}. Figure \ref{Line_Conj_Pars} shows the hierarchical structure of the required parameters for linear conjunctive model. This graph can be extended to other models. 

\begin{figure}[ht]
\centering
 \includegraphics[trim= 3cm 15cm 5cm 3cm, scale =0.85] {Linear_Conj_Pars.pdf}\label{PerfectQ}
\caption{hierarchical structure of parameters of linear conjunctive model}
\label{Line_Conj_Pars}
\end{figure}

There exist some intermediate steps between the lower level(simple) and higher level(more complex) parameters which can potentially result in a variance in the generated data. The goal of creating this package is to efficiently use the available parameters to generate data based on different models where potential conflicts and exceptions are covered. 
