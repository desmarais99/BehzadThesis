% Abstract
%
%   Résumé de la recherche écrit en anglais sans être
% une traduction mot à mot du résumé écrit en français.

\chapter*{ABSTRACT}\thispagestyle{headings}
\addcontentsline{toc}{compteur}{ABSTRACT}
%
\begin{otherlanguage}{english}

In educational data mining, or in data mining in general, analysts that wish to build a classification or a regression model over new and unknown data are faced with a very wide span of choices.  Machine learning techniques nowadays offer the possibility to learn and train a large and an ever growing variety of models from data. Along with this increased display of models that can be defined and trained from data, comes the question of deciding which are the most representative of the underlying ground truth.  The main objective of this thesis is assessing model fit for student test results.

Assessing whether a student model is a good fit to the data is non trivial.  The standard practice is to train different models, and consider the one with the highest predictive performance as the best fit. But each model may involve different machine learning algorithms that carry their own set of parameters and constraints imposed on the corresponding model.  This results in a large space in which to explore model performance.  The actual best fitting model may have been overlooked due to an unfortunate choice of the algorithm's parameters.  Therefore, the best performer may not be the model that is most representative of the ground truth, but instead it may be the result of contextual factors that make this model outperform the ground truth one.  

We investigate the question of assessing different model fits using synthetic data by defining a vector space based of model performances, and use a nearest neighbor approach on the bases of correlation to identify the ground truth model. The results show that some similar models that represent almost the same concept show pretty much good correlation but for two different synthetic dataset with the same underlying model there is a high correlation. For those that have completely different model it is low. 

Considering this approach and ``best performer'' as two classification approaches for model fitting, results show that the approach is more accurate than the ``best performer'' approach, but only for some ground truth models.

Also we discuss the stability of the model performance vector in the space and its uniqueness for different data generation parameters such as sample size, average success  rate, number of skills, number of items, examinee and item variance. Results of this experiment show that the performance vector is sensitive to some data generation parameters. Some parameters like number of skills has a tangible effect on model performance pattern unlike some others such as sample size. Generally the pattern slightly changes through predictive performances. 


\end{otherlanguage}
