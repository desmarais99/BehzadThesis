% Abstract
%
%   Résumé de la recherche écrit en anglais sans être
% une traduction mot à mot du résumé écrit en français.

\chapter*{ABSTRACT}\thispagestyle{headings}
\addcontentsline{toc}{compteur}{ABSTRACT}
%
\begin{otherlanguage}{english}

In educational data mining, or in data mining in general, analysts that wish to build a classification or a regression model over new and unknown data are faced with a very wide span of choices.  Machine learning techniques nowadays offer the possibility to learn and train a large and an ever growing variety of models from data. Along with this increased display of models that can be defined and trained from data, comes the question addressed in this thesis: how to decide which are the most representative of the underlying ground truth.

The standard practice is to train different models, and consider the one with the highest predictive performance as the best fit. However, model performance typically varies along factors such as sample size, target variable and predictor entropy, noise, missing values, etc.  For example, a model's resilience to noise and ability to deal with small sample size may yield better performance than the ground truth model for a given data set.  

Therefore, the best performer may not be the model that is most representative of the ground truth, but instead it may be the result of contextual factors that make this model outperform the ground truth one.  

We investigate the question of assessing different model fits using synthetic data by defining a vector space of model performances, and use a nearest neighbor approach with a correlation distance to identify the ground truth model.  This approach is based on the following definitions and procedure.  Consider a set of models, $\mathcal{M}$, and a vector~$\mathbf{p}$ of length~$|\mathcal{M}|$ that contains the performance of each model over a given data set.  This vector represents a point that characterize the data set in the performance space.  For each model $M \in \mathcal{M}$, we determine a new point in the performance space that corresponds to synthetic data generated with model~$M$.  Then, for a given data set, we find the nearest synthetic data set point, using correlation as a distance, and consider the model behind it to be the ground truth.

The results show that, for synthetic data sets, their underlying model sets are generally more often correctly identified with the proposed approach than by using the best performer approach.  They also show that semantically similar models are also closer together in the performance space than the models that are based on highly different concepts.

% Also we discuss the stability of the model \textit{performance vector} in the space and its uniqueness for different data generation parameters such as sample size, average success  rate, number of skills, number of items, examinee and item variance. Results of this experiment show that the \textit{performance vector} is sensitive to some data generation parameters. Some parameters like number of skills has a tangible effect on model performance pattern unlike some others such as sample size. Generally the pattern slightly changes through predictive performances. 


\end{otherlanguage}
