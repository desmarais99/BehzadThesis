\Chapter{PERFORMANCE SIGNATURE APPROACH}\label{sec:Approach}

This chapter presents the method we introduce to determine the model that is closest to the ground truth for a given dataset. We refer to this method as ``Performance Signature'' approach, or ``Signature'' approach for short. 

The basic principles of the method is to define a vector space of model performances, namely the models described in chapter \ref{sec:RevLitt}. Then, synthetic data is created with each model, and each model's performances are measured over each of the synthetic dataset. These measures represent a point in that space which is called \textit{Performance prototype}. The synthetic data point associated with that point represents the ground truth prototype, or the centroid for classification. Given a new dataset, a \textit{Target Performance Vector} can be obtained. Then, we use a nearest neighbor approach with a correlation distance to identify the closest ground truth model. Figure \ref{SignatureFrameworkNew} demonstrates these steps in the signature framework.

\begin{figure}[h]
\centering
{\includegraphics[trim=1cm 10cm 1cm 1cm,scale=0.7]{images/Approach4.pdf}}
\caption{Signature framework}
\label{SignatureFrameworkNew}
\end{figure}


Let us rephrase the method in more specific terms and consider a set of models, $\mathcal{M}$, and a vector~$\mathbf{p}$ of length~$|\mathcal{M}|$ that contains the performance of each model over a given dataset. This \textit{Target Performance Vector} represents a point in the performance space. For each model $M \in \mathcal{M}$, we determine a \textit{Performance prototype} point in the performance space that corresponds to synthetic data generated with model~$M$. Then, for a given dataset, we find the nearest synthetic dataset point, using correlation as a distance, and consider the model behind it to be the ground truth. Next section provides practical examples of this framework.


The rest of the chapter goes into more details about the method.

\section{Model fit in a vector space framework}

The performance data in table~\ref{tab:vectorspace} will serve here to explain the method with a concrete example. In this table, the predictive accuracy of the 6~models reviewed in chapter~\ref{sec:RevLitt} is reported against 6~synthetic datasets generated with the same models. A seventh ``model'' named \textit{expected} (section~\ref{sec:basel-expect-pred}) and a seventh dataset named \textit{random} (random results that constrained to reflect the target dataset parameters such as item and student success rate distribution) are added for better comparison purpose. 

\newcolumntype{d}[1]{D{.}{.}{#1}} % not used

\begin{table}[ht]
\caption{Vector space of accuracy performances}\label{tab:vectorspace}
\centering
\begin{tabular}{lcccccccc}
\cline{1-9}
  \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{7}{c}{\textbf{Performance prototype }} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\textcolor{red}{Target}}}}\\
  \cline{2-8}
  & \multicolumn{1}{c}{{\textit{Random}}} & \multicolumn{1}{c}{{POKS}} & \multicolumn{1}{c}{{IRT}} & \multicolumn{1}{c}{{DINA}} & \multicolumn{1}{c}{{DINO}} & \multicolumn{1}{c}{{L.Conj.}} & \multicolumn{1}{c}{{L.Comp.}} &\\ 
\cline{1-9}
  \textit{Expected} & \textbf{0.75} & 0.91 & 0.90 & 0.72 & 0.72 & 0.78 & 0.93 & \textcolor{red}{0.43} \\ 
  POKS & 0.75 & \textbf{0.94} & 0.94 & 0.81 & 0.81 & 0.90 & 0.94  & \textcolor{red}{0.75}\\ 
  IRT & 0.75 & 0.91 & \textbf{0.95} & 0.73 & 0.73 & 0.79 & 0.89  & \textcolor{red}{0.68}\\ 
  DINA & 0.75 & 0.77 & 0.81 & \textbf{1.00} & 0.65 & \textbf{0.98} & 0.89  & \textcolor{red}{0.93}\\ 
  DINO & 0.75 & 0.63 & 0.56 & 0.66 & \textbf{1.00} & 0.68 & 0.91  & \textcolor{red}{0.60}\\ 
  NMF.Conj & 0.75& 0.59 & 0.53 & 0.95 & 0.65 & 0.97 & 0.58  & \textcolor{red}{0.80}\\ 
  NMF.Comp & 0.75 & 0.76 & 0.79 & 0.59 & 0.93 & 0.70 & \textbf{0.98}  & \textcolor{red}{0.70}\\ 
\cline{1-9}
\end{tabular}
\end{table}



As we can expect, the diagonal (in bold face, except for one, corresponding to the match between the underlying synthetic model and the model performance) generally displays the best performance since it corresponds to the alignment of the model and the ground truth behind the data. This confirms the intuition behind the usual strategy of assuming the best performer is the model behind the ground truth. However, this is not always the case. In this particular example, the DINA model shows a better performance over the Linear Conj.\ dataset, whereas this dataset's corresponding model is the NMF Conj. 

The principle of the proposed approach is to use the whole column of performance as a vector to determine the closest model to the ground truth of target performance vector. In that respect, if columns are considered as vectors in the space of dimensions created by model performances, we can use a similarity measure to determine the closest ground truth to the target performance vector (or a distance measure if we consider the columns as a point in space).

The advantage of this approach is that it does not rely on a single performance value to determine the goodness of fit, but instead on a set of performances over different models. The hypothesis is that this set of performances provides a more reliable measure of the goodness of fit of a set of models. In turn, we assume that this measure is more likely to indicate which model will perform better in general, as opposed to which models performs the best in the case of the single dataset at hand.

The approach can be considered as a means to avoid a kind of local minimum, considering the best performer as a good indicator of the ground truth, but not a perfect one. Indeed, table~\ref{tab:vectorspace} suggests that aligning the model with the ground truth does yield the best performance except for one case, but we will show more examples later that there are exceptions and that the proposed approach is better able to avoid these exceptions that would lead to a wrong conclusion if we were to rely on the best performer approach.

\section{Research questions}

Let us get back to the research questions and design implementation of experiments to answer them, This section explains a general framework for each experiment, later in chapter \ref{sec:SIGNATURE} the results of these experiment with more details are given. To make it straightforward we break the contribution into four experiments:

\begin{enumerate}
\item What is the \textit{performance vector} of student skills assessment models over real and over synthetic data created using the same models?
\begin{itemize}
\item Experiment 1: Predictive performance of models over real and synthetic datasets: In a first experiment, we focus on showing the performance of all models described in chapter \ref{sec:RevLitt} over synthetic and real datasets. It provides an overview of the predictive performance of each model across the different synthetic and real datasets. The output of this experiment is the \textit{Performance vector} or the \textit{Performance signature} of a dataset.
\end{itemize}
\item Is the \textit{performance vector} unique to each synthetic data type (data from the same ground truth model)?
\begin{itemize}
\item Experiment 2: Sensitivity of the signature over different data specific parameters: This section tests the uniqueness of the \textit{performance signature} across different data generation parameters.
\end{itemize}
\item Can the \textit{performance vector} be used to define a method to reliably identify the ground truth behind the synthetic data?
\begin{itemize}
\item Experiment 3: Model selection based on \textit{performance vector} classification: This part proposes a framework for model selection as well as a measure that represents a measure for the goodness of fit of a model by comparing the target \textit{performance vector} of a real dataset and the \textit{performance prototype} vectors obtained from each synthetic datasets.
\end{itemize}
\item How does the method compare with the standard practice of using the model with the best performance?
\begin{itemize}

\item Experiment 4: Nearest neighbor vs. best performer classification: Assume that the measure for the goodness of fit in experiment 3 is a good indicator of model fit for synthetic data. In a last experiment we want to evaluate the ability of signature approach to identify the ground truth model on synthetic data and also compare the results with the simple best performer approach. In fact, we want to test the generality of the approach by classifying datasets with different data specific parameters in the \textit{performance vector} space. 

To validate the approach, we need to rely on synthetic data for which we know the underlying ground truth model. A matrix is created with datasets that are generated from the different models, and each model performance is measured through a cross validation process (using experiment 1). This matrix allows us to classify a dataset of unknown ground truth according to the nearest neighbor approach (using experiment 3). Finally we compare these results with the best performer classification results.

\end{itemize}
\end{enumerate}

\subsection{Experiment 1: Predictive performance of models over real and synthetic datasets}
\label{Exp1:NO}
The performance of each model is assessed on the basis of 10-folds cross-validation. The training set is used to estimate model parameters that are later used in for the test set. For each test set, a model is fed with a set of item outcomes of a student, called the observed set, and the remaining items are the predicted, or inferred ones. The breakdown of the data for cross-validation is illustrated in figure~\ref{figMethod}. 

\begin{figure}[h]
\centering
{\includegraphics[trim=1cm 9cm 2.4cm 2.4cm,clip=true,width=.7\textwidth]{images/Methodology.pdf}}
\caption{Data breakdown of cross validation process}
\label{figMethod}
\end{figure}


For each dataset there exists a training set that contains 9 folds and a test set, which represents a single fold. Samples are assigned randomly to each fold and this setting is the same across all predictive models for each run. Since all items are presented in the training set, then we can estimate the parameters that are related to items to be used in the test set. 

For other parameters that are related to students we need to divide the test set into an observed and inferred set. From the observed set we can estimate these parameters. Obviously, the number of observed items is an important factor that can affect the predictive performance and consequently it can change the performance vector. Given a test outcome, $D$, with a set of total items, $\mathcal{I}$, and a subset of randomly selected items to be observed which are fed to the model, $\mathcal{I}_o$, consequently the remaining subset are target items to predict, $\mathcal{I}_t = \mathcal{I}\setminus\mathcal{I}_o$. In order to cover different number of observed items in the predictive performance of a model over a dataset, we report the average of performances with four different numbers for observed items:

\begin{equation}
P = \frac{\sum_{i = 1}^{4}{P(D_{q_i})}}{4}
\label{Eq:Overal-performance}
\end{equation} 

 where $P$ is the average performance of a model over dataset~$D$; and $q_i$ varies between the 3-quantiles (tertiles) of natural numbers in the range of $5$ to $(|\mathcal{I}|-1)$; and $P(O_{q_i})$ is the performance of a model with $q_i$ observed items.

The observation set, $\mathcal{I}_o$, is chosen to cover all items in the item set $\mathcal{I}$. For example, with $|\mathcal{I}| = 20$ and $|\mathcal{I}_o| = 5$ we randomly divide $\mathcal{I}$ into $4$ subsets of observable items while the union of them recovers all items in $\mathcal{I}$. Consider $\mathcal{I}_o^n$ with $n$ items to be observed, there are ${|\mathcal{I}|\choose n}$ combinations to obtain this subset from $\mathcal{I}$. Let us name $i^{th}$ possibility as $\mathcal{I}_{o_i}^n$. Therefore:
\begin{equation}
P(D_n) = \frac{\sum_{j = 1}^{|\mathcal{I}|\choose n}{P(D_{\mathcal{I}_{o_j}^n})}}{{|\mathcal{I}|\choose n}}
\label{Eq:Sub-Performance}
\end{equation}

Combining equation \ref{Eq:Overal-performance} with \ref{Eq:Sub-Performance}, one can calculate the average performance of a dataset, $P$ in equation \ref{Eq:Overal-performance}, that considers a variety of observation where $n$ in \ref{Eq:Sub-Performance} equals to $q_i$ in \ref{Eq:Overal-performance}.

A list of required model specific parameters for assessing the model performance is presented in table~\ref{fig:param-Predictive-Performance} \footnote{Details of all parameters are given in chapter \ref{sec:Syn}}.

\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\newcommand\VRule[1][\arrayrulewidth]{\vrule width #1}

\begin{table}[h]
  \centering

\begin{tabular}{c|c|l!{\VRule[1.5pt]}l|l!{\VRule[1.5pt]}l|}
\multicolumn{3}{c}{}&\multicolumn{3}{c}{Parameters estimated from}\tabularnewline
\cline{4-6}
\multicolumn{3}{c!{\VRule[1.5pt]}}{Skills Model}&\multicolumn{2}{c!{\VRule[1.5pt]}}{Training set}&Observed items\tabularnewline
\cmidrule[1.5pt]{2-6}
&&NMF Conj. & & &  \tabularnewline
\cline{3-3}
&&NMF Add.&&& \tabularnewline
\cline{3-4}
&&DINA& \tabitem Slip & &\tabularnewline
\cline{3-3} 
&\multirow{-4}{*}{\begin{sideways} \scriptsize Multiple\end{sideways}}&DINO& \tabitem Guess&\multirow{-4}{*}{ \tabitem  Q-matrix }&\multirow{-4}{*}{  \parbox[t]{3cm}{ \tabitem Students skills  \\ mastery matrix}} \tabularnewline
\cmidrule[1.5pt]{2-6}
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Item difficulty  } &  \tabularnewline
&&\multirow{-2}{*}{ IRT}&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Item discrimination}  &\multirow{-2}{*}{\tabitem Student Ability} \tabularnewline
\cline{3-6}
&\multirow{-3}{*}{\begin{sideways} \scriptsize Single \end{sideways}}&Expected&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Item Odds}  &\tabitem Student Odds \tabularnewline
\cmidrule[1.5pt]{2-6}
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Initial Odds } & \tabularnewline
&&&\multicolumn{2}{l!{\VRule[1.5pt]}}{ \tabitem Odds ratio } & \tabularnewline
\multirow{-9}{*}{\begin{sideways} \scriptsize Contributed skills \end{sideways}}&\multirow{-3}{*}{\begin{sideways} \scriptsize Zero\end{sideways}}&\multirow{-3}{*}{ POKS }&\multicolumn{2}{l!{\VRule[1.5pt]}}{\tabitem Partial order structure}&\multirow{-3}{*}{ }\tabularnewline
\cmidrule[1.5pt]{2-6}
\end{tabular}
  \caption{Parameters of the predictive performance framework}
  \label{fig:param-Predictive-Performance}
\end{table}

Once all the required parameters are presented we can make a prediction for the inferred cells of the result matrix. Note that the selected observed and inferred items are the same across all the models for each run to make a better comparison for their prediction. A probability of mastery is obtained and rounded, resulting in a 0/1 error loss function. We report the mean accuracy as the performance measure later in chapter \ref{sec:SIGNATURE}. The R package \texttt{ltm} is used for parameter and skills estimation for IRT model and the R package \texttt{CDM} and \texttt{NMF} for Deterministic Input Noisy and NMF models. 

Recall that this chapter provides a description of experiments whose results are presented later in chapter \ref{sec:SIGNATURE}.

\subsection{Experiment 2: Sensitivity of the Model performance over different data generation parameters}
\label{Sensitive}

In this experiment we want to examine the effect of data specific parameters on the stability of the \textit{performance prototype vector} in the performance space. We run the same experiment as before but with different contextual data generation parameters such as average success rate, sample size, number of latent skills, number of items, student and item score variance. This experiment can answer the question whether the pattern of \textit{performance signatures} for datasets with the same ground truth remain stable across different conditions. Also it can answer whether the synthetic datasets to assess model fit should follow the same data specific parameters as the real data.


\subsection{Experiment 3: Model selection based on \textit{performance vector} classification}
\label{Sigapproach-measure}
This experiment essentially introduces the ``signature'' approach. By the assumption that previous experiment proves that the uniqueness characteristic of synthetic data signatures makes it easy to identify the ground truth and also that there exists some similarity between the pattern of \textit{performance signature} of synthetic and real datasets (chapter \ref{sec:SIGNATURE} explains these results). Hence we can define a measure for the similarity of a synthetic generated dataset and a real one given a set of candidate models. Given that our objective is to determine the ground truth of a given dataset, we will borrow data specific parameters from this dataset to generate the synthetic data. Further details on data generation are given in chapter~\ref{sec:Syn}. 

As explained before, signature approach search for the nearest neighbor of the target \textit{performance vector} among the \textit{performance prototypes}. The principle of this search is to find the closest \textit{performance prototype} to the \textit{target performance vector} in the performance space. Since this search is in a hyper space we used a linear discriminant approach which is the nearest neighbor classifier. This approach essentially splits the space into sub-spaces with defined surfaces. However, there exist other approaches to define a surface such as SVM \citep{cortes1995support,suykens2002least}, single layer neural network \citep{lippmann1987introduction} or LDA \citep{mclachlan2004discriminant,scholkopft1999fisher}. We do not directly define any surface in the performance space but we just measure the closest neighbor. In this experiment we used Pearson correlation coefficient as a measure of similarity to find the nearest neighbor.

There are two approaches to search for the nearest neighbor in this space:
\begin{itemize}
\item \textbf{Centroid performance prototype}: In total there are $7$ neighbors(points in performance space) for this search where each of them is a representative of a performance prototype of a skills assessment ground truth (Chapter \ref{sec:RevLitt} explains these models in details). Each centroid performance prototype is defined as the average of \textit{performance vectors} of $10$ datasets generated with the same model and data specific parameters but different seeding points. The nearest neighbor among these $7$ centroid \textit{performance vectors} becomes the estimated ground truth.
\item \textbf{Majority voting among 10 nearest neighbors}: This approach considers all neighbors in the search without defining a centroid point for each performance prototype. In this case the distance between each neighbor's \textit{performance vector} and the \textit{target performance vector} is calculated and a majority voting among the ground truth of $10$ nearest neighbors defines the estimated ground truth.
\end{itemize}

In this experiment we used the first approach to define the closest neighbor. However, both approaches show identical results.

%%%%%%%%H%%%%
\subsection{Experiment 4: Signature vs. best performer classification}
\label{Classification}

The very last experiment is to test the generality of signature approach to assess a model fit over different data specific parameters. Despite the fact that different data specific parameters may affect the performance signature (as we will see in the results of experiment 2), this experiment tests the reliability of this approach to estimation of the ground truth on datasets with different data specific parameters. We also want to compare the results with the simple best performer approach. Akin to experiment 2, this experiments relies on synthetic datasets since the ground truth and data specific parameters are known. 

For this purpose a number of synthetic datasets are generated with different data specific parameters. Considering $6$ data specific parameters which are presented in table \ref{fig:param-Predictive-Performance} and $4$ different values for each parameter, while others have default values. This results in $24$ different conditions for the data generation process. The goal of this experiment is to obtain the accuracy of signature approach to classify any dataset with each of these $24$ conditions. Therefore, with each condition we generate $10$ datasets with different seeding point for each skills assessment ground truths. This results in $60$ datasets for each condition. let us name this set of $60$ datasets as a \textbf{group}. Totally $24$ \textbf{groups} can be generated.

The classification is preforming among the members of a \textbf{group} where all datasets share the same data specific parameters. We randomly select a dataset whose \textit{performance vector} becomes the \textit{target performance vector}. Consequently the other $59$ datasets in the \textbf{group} are neighbors with \textit{performance prototype} vectors. As explained in experiment 3 we can calculate the distance between the \textit{target performance vector} and each of neighbors for the purpose of model selection. Finally a majority voting among the ground truth of 10 nearest neighbors defines the ground truth. This process repeats for each \textbf{group}.

Obviously the highest performance in a \textit{performance vector} is the classification result for the ``best performer'' approach. Later in section \ref{GeneralityRes} we use the classification measures from section \ref{MeasuresT} to compare the reliability of these two approaches.
