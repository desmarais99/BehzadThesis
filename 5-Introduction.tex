\selectlanguage{english}
\Chapter{INTRODUCTION}\label{sec:Introduction} 

\section{Problem Definition and Challenges}
\paragraph{}
\selectlanguage{english} 

In Educational Data Mining, or in Data Science in general, analysts that wish to build a classification or regression model over new and unknown data are faced with a very wide span of choices.  Machine learning techniques nowadays offer the possibility to learn and train a large and an ever growing variety of models from data.  Learning techniques such as the E-M algorithm and MCMC methods have contributed to this expansion of models we can learn from data.  They allow model parameters estimation that would otherwise represent an intractable problem using standard analytical or optimization techniques.

Along with this increased display of models that can be defined and trained from data, comes the question of deciding which are the most representative of the underlying ground truth.  This question is of interest from two perspectives.  One is the theoretical and explanatory value of uncovering a model that accounts for observed data.  The other perspective is the assumption that the ``true'' underlying model will better generalize to samples other than the training data.  This assumption is commonly supported in physics where some models have a window in the parameter space where they correctly account for observations, and break down outside that window; Newtownian and modern physics are prototypical examples supporting this assumption.  

In the machine learning field, the case for the support of the assumption that the closer to the ground truth a model is, the better it will generalize outside the parameter space, is not as evident as it can be in physics. But we do find analogous examples such as the Naïve Bayes classifier under a 0-1 loss function tend to perform very well in spite of the unrealistic assumption of the naïve independence assumption at the root of the approach's name \citep{domingos1997}.

Given that in machine learning, we are often more interested in the predictive power of models than we are in their theoretical and explanatory value, the standard practice is to choose the model with the best predictive performance.  And without good theoretical understanding of the domain, we simply hope that it will generalize outside the space covered by our training sample.  

This thesis aims to provide a means to assess the fit of the model to the underlying ground truth using a methodology based on synthetic data, and to verify if the approach is better able to identify a model that will generalize outside the parameter space of the training sample.  The study is circumscribed to the domain of Educational Data Mining where we find numerous competing models of student skills mastery.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model selection and goodness of fit}

Model selection is the task of selecting a statistical model for a given data from a set of candidate models.  Selection is most often based on a model's ``goodness of fit''.

On the hand the term ``goodness of fit''  for a statistical model describes how well it fits a set of observation. The distance between observed values and the predicted values under the model can be a measure of goodness of fit. The goodness of fit is usually determined using likelihood ratio. There exists different approaches to assess model fit based on the measure of goodness of fit. The consensus is that the model with the best predictive performance is the most likely to be the closest to the ground Truth. Then there are the issues of how sensitive is the model to sample size, noise, and biases that also need to be addressed before we can trust that this model is the best candidate. It can take numerous studies before a true consensus emerges as to which model is the best candidate for a given type of data.

Another approach to assess which model is closest to the ground truth is proposed in this thesis. It relies on the analysis of model predictive performances over real data and synthetic data. Using synthetic data allows us to validate the sensitivity of the model selection approach to data specific parameters such as sample size and noise. Comparing performance over synthetic and real data has been used extensively to validate models, but we further elaborate on the standard principle of comparison over both types of data by contrasting the predictive performance across types of synthetic data.  The hypothesis we make is that the relative performance of different models will be stable by the characteristic of a given type of data, as defined by the underlying ground truth for real data, or by the model that generates the synthetic data.  We explore this hypothesis in the domain of Educational Data Mining and the assessment of student skills, where a set of latent skills are mapped to question items and students skill mastery is inferred from item outcome results from test data. 


This chapter introduces and defines these concepts, as well as outlines the objectives and main scientific hypotheses of the proposed research. The final section presents the organization of the remainder of this research.

\section{Thesis vocabulary}

In this section we introduce a vocabulary that is related to the general objective of this thesis and used in all chapters:

\begin{itemize}
\item \textbf{Student model}: "In general terms, student modeling involves the construction of a qualitative representation that accounts for student behavior in terms of existing background knowledge about a domain and about students learning the domain. Such a representation, called a student model" \citep{sison1998student}. Student skills assessment models are essentially constructed to assess student's skills or estimate potential skills required for problems. Since our experiments are in the domain of educational data mining, by the term ``Model'' we mean ``student model''. 
\item \textbf{Dataset (Real/Synthetic)}: Dataset in this context represents student test outcome which is a matrix that shows the result of a test given by students. A test is simply a set of few questions, problems or items that can have a success of a failure result in the dataset. Datasets can be ``real'' or ``synthetic''. A ``Real'' dataset is the result of an actual test given by individuals in an e-learning environment or even a classroom. The term ``Synthetic'' means that a simulation is involved to generate an artificial student test outcome. The simulation is designed based on a model that takes a set of predefined parameters to generate student test outcome. This set contains two types of parameters: Model specific parameters and Data specific parameters.
\item \textbf{Model specific parameters}: These parameters are specifically defined and learnt based on model's type. Complex models contain more parameters. Some models may share some parameters but some models have no parameters in common.
\item \textbf{Data specific parameters}: These parameters are common between all datasets such as average success rate, sample size and number of items in a dataset.
\item \textbf{Ground truth}: This term is originally coined by Geographical/earth science where if a location method such as GPS estimates a location coordinates of a spot on earth, then the actual location on earth would be the ``Ground truth''. This term has been adopted in other fields of study. In this context ``ground truth of a dataset'' means the actual model that best describes the dataset within its parameters.  Note that for the skills models studied here, the ground thruth is always unknown, unless we use synthetic data.
\item \textbf{Performance of a model}: The accuracy of a model to predict student response outcomes over a dataset (using cross-validation) is called performance of a model.  Different models have different performances over a dataset. Assessing such a performance requires designing an experiment to learn the model's parameters and predict a proportion of the dataset that has not been involved in the learning phase.
\item \textbf{Performance vector}: Each model has a performance over a dataset. Consider a set of models, $\mathcal{M}$, and a vector~$\mathbf{p}$ of length~$|\mathcal{M}|$ that contains the performance of each model over a given data set.  This vector represents a point in the performance space, and it is defined as the \textit{performance vector} of that data set.
\item \textbf{Performance Signature}: We use the term \textit{performance signature} to designate the performance vector associated with the synthetic data of a model.  Note that there are different ways to prouduce the synthetic data of a model and we return to this question later.

The performance vector can be considered from two perspectives: The first perspective is a performance vector in the performance space where we have the same number of dimensions as the number of candidate models in the performance vector. The second one is a kind of ``signature'' for a specific data which considers the vector in a two dimensional space with performances on the \textit{y} axis and skills assessment models on \textit{x} axis.  They are sharing the same concepts but different presentations. 




\end{itemize}

\section{Research Questions}
\paragraph{}The following questions are addressed in this thesis:

\begin{enumerate}
\item What is the performance vector of student skills assessment models over real and over synthetic data created using the same models?
\item Is the performance vector unique to each synthetic data type (data from the same ground truth model)?
\item Can the performance vector be used to define a method to reliably identify the ground truth behind the synthetic data?
\item How does the method compare with the standard practice of using the model with the best performance?  In particular, does the ground truth model identified better generalize over a space of parameter values?
\end{enumerate}


\section{General Objectives}

\paragraph{}The general objective of this thesis is to assess the goodness of fit on the basis of the what we will refer to as performance signatures. It can be divided in three sub-objectives: The first objective is to obtain the performance signatures of skills assessment models over a synthetic datasets generated with these very same models. This will create a vector of performances in the performance space. The second one is to assess model fit using the performance vector of the synthetic and real data. The third objective is to test the uniqueness and sensitivity of the performance vectors on the different data specific conditions such as sample size, nose, average success rate.

\section{Hypotheses}
\paragraph{}The research in this thesis tests the following hypotheses:
\paragraph{Hypothesis 1:} The performance vectors of two datasets with the same ground truth have a high level of correlation.
\paragraph{Hypothesis 2:} The best performer model is not necessarily the ground truth model.
\paragraph{Hypothesis 3:} Datasets with the same model parameters and data specific parameters create unique performance vector. %\note{Behzad says: or The predictive performance of each synthetic data type is unique} \note{Behzad says:or the predictive performance of different models will be stable by the characteristic of a given type of data}
\paragraph{Hypothesis 4:} Datasets with the same ground truth but different data specific parameters can have different performance vectors. 
\note{Do we need hypothesis 3 and 4?}
\par\note{Should we have a hypothesis about the expected performance of the ground thruth model compared to the best performer?  Something like: ``If the ground thruth model is different than the best performer model for a given data set, the expected value of the performance of the ground thruth model over the space of data and model parameters is greater''.  And if we made this hypothesis, do we have the demonstration that it is a false hypothesis, as we concluded in a recent discussion?}



\section{Main Contributions}

The main contribution of this thesis is assessing model fit of a data set by comparing its performance vector to the performance signatures of synthetic and real data. This method can be applied to different fields of studies but in this research we focus on student test result and on a few skills assessment models that have emerged mostly in EDM and ITS. The predictive performance of each model is assessed by designing an experiment which learns the model parameters and observes a set of items for a student to predict the rest of items test results of that student. The mean predictive accuracy will be the predictive performance measure. Previous researches compared their predictive performance  on a pairwise basis, but few studies have taken a comprehensive approach to compare them on a common basis. In this research we used seven skills assessment models to obtain the predictive performance vector using the same models. 

The next step is to use this performance vector to assess model fit for a real dataset. The standard practice is to pick the ``best performer'' as the ground truth model. The actual best fitting model may have been overlooked due to an unfortunate estimate of the algorithm's parameters.  Therefore, the best performer may not be the model that is most representative of the ground truth, but instead it may be the result of contextual factors that make this model outperform the ground truth one. We investigate the question of assessing different model fits using synthetic data by defining a vector space based on model performances, and use a nearest neighbor approach on the bases of correlation to identify the ground truth model. Comparing the performance of synthetic dataset with a specific underlying model and the performance of a real dataset with the same underlying model should show a high correlation. 

Still the question of sensitivity of the ``signature'' to contextual factors should be considered in the comparison of the performance vectors. The other contribution is to test the stability of the ``signature'' of synthetic datasets over different data specific parameters (such as sample size, average success rate, etc.) generated with the same underlying model. 


\section{Publications}

Along the course of the doctorate studies, I contributed to a number of publications, some of which are directly related to this thesis, and some of which are peripheral or are preliminary studies that led to the thesis.

\begin{enumerate}

\item \textbf{B. Beheshti}, M.C. Desmarais, “Assessing Model Fit With Synthetic vs. Real Data" , Journal Submitted to \textbf{Journal of Educational Data Mining}.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Goodness of Fit of Skills Assessment Approaches: Insights from Patterns of Real vs. Synthetic Data Sets", Short Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain, pp: 368-371.

\item \textbf{B. Beheshti}, M.C. Desmarais, R. Naceur, “Methods to Find the Number of Latent Skills”, short paper in \textbf{International Educational Data Mining 2012} July 2012, Crete, Greece. , pp: 81-86.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Improving matrix factorization techniques of student test data with partial order constraints", Doctoral consortium in \textbf{User Modeling, Adaptation, and Personalization 2012} Aug 2012, Montreal, Canada. , pp: 346-350.

\item M.C. Desmarais, \textbf{B. Beheshti}, P. Xu, “The refinement of a q-matrix: assessing methods to validate tasks to skills mapping", Short paper in \textbf{International Educational Data Mining 2014} June 2014, London, United Kingdom., pp: 308-3011.

\item M.C. Desmarais, \textbf{B. Beheshti}, R. Naceur, “Item to skills mapping: deriving a conjunctive q-matrix from data”, short paper in \textbf{Intelligent Tutoring Systems 2012} July 2012, Crete, Greece. , pp: 454-463.

\item M.C. Desmarais, P. Xu, \textbf{B. Beheshti}, “Combining techniques to refine item to skills Q-matrices with a partition tree", Full Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain., pp: 29-36.

\item M.C. Desmarais, R. Naceur, \textbf{B. Beheshti}, “Linear models of student skills for static data", Workshop in \textbf{User Modeling, Adaptation, and Personalization 2012} July 2012, Montreal, Canada.
\end{enumerate}


\section{Organization of the Thesis}
\paragraph{} We review the related literature on fundamental concepts in Educational Data Mining and some machine learning techniques that have been used in our experiments in Chapter \ref{sec:RevLitt}. Chapter \ref{sec:RelatedWorks} discusses recent work about model selection. The main contribution of the research starts from chapter \ref{sec:Approach} where we explain the proposed approach in details. As a complementary part of the proposed approach we explain synthetic data generation approaches in chapter \ref{sec:Syn}. The experimental results of the main contribution are explained in details in Chapter \ref{sec:SIGNATURE}. Finally, we conclude and outline future work in Chapter \ref{sec:Conclusion}. 


