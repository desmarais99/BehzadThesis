\selectlanguage{english}
\Chapter{INTRODUCTION}\label{sec:Introduction} 

\section{Problem Definition and Challenges}
\paragraph{}
\selectlanguage{english} 

Data analysts that wish to build a classification or regression model over new and unknown data are faced with a very wide span of choices.  Machine learning techniques nowadays offer the possibility to learn and train a large and an ever growing variety of models from data.  Learning techniques such as the E-M algorithm and MCMC methods have contributed to this expansion of models we can learn from data.  They allow model parameters estimation that would otherwise represent an intractable problem using standard analytical or optimization techniques.

Along with this increased display of models that can be defined and trained from data, comes the question of deciding which are the most representative of the underlying ground truth.  This question is of interest from two perspectives.  One is the theoretical and explanatory value of uncovering a model that accounts for observed data.  The other perspective is the assumption that the ``true'' underlying model will better generalize to samples other than the training data.  This assumption is commonly supported in physics where some models have a window in the parameter space where they correctly account for observations, and break down outside that window; Newtownian and modern physics are prototypical examples supporting this assumption.  

In the machine learning field, the case for the support of the assumption that the closer to the ground truth a model is, the better it will generalize outside the parameter space, is not as evident as it can be in physics. But we do find analogous examples such as the Naïve Bayes classifier under a 0-1 loss function tend to perform very well in spite of the unrealistic assumption of the naïve independence assumption at the root of the approach's name \citep{domingos1997optimality}.

Given that in machine learning, we are often more interested in the predictive power of models than we are in their theoretical and explanatory value, the standard practice is to choose the model with the best predictive performance.  And without good theoretical understanding of the domain, we simply hope that it will generalize outside the space covered by our training sample.  

This thesis aims to provide a means to assess the fit of the model to the underlying ground truth using a methodology based on synthetic data, and to verify if the approach is better able to identify a model that will generalize outside the parameter space of the training sample.  The study is circumscribed to the domain of Educational Data Mining where we find numerous competing models of student skills mastery.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model selection and goodness of fit}

Model selection is the task of selecting a statistical model for a given data from a set of candidate models. Both data and candidate models involve in the design of an experiment which analyses the given data statically. Even selecting candidate models should be done properly where a model that potentially can represent the dataset within its parameters should be considered as a candidate. Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice. The complexity of the model can be presumed as the number of parameters it contains. The more complex the model is, the better its parameters are learnt to fit the data which also causes over-fitting problem.


On the hand the term ``Goodness of fit''  for a statistical model describes how well it fits a set of observation. The distance between observed values and the predicted values under the model can be a measure of goodness of fit. The goodness of fit is usually determined using likelihood ratio. There exists different approaches to assess model fit based on the measure of goodness of fit. The consensus is that the model with the best predictive performance is the most likely to be the closest to the ground Truth. Then there are the issues of how sensitive is the model to sample size, noise, and biases that also need to be addressed before we can trust that this model is the best candidate. It can take numerous studies before a true consensus emerges as to which model is the best candidate for a given type of data.


Yet another approach to assess which model is closest to the ground truth is to combine predictive performance analysis of real data with synthetic data. Using synthetic data allows us to validate the sensitivity of the model selection approach to data specific parameters such as sample size and noise. Comparing performance over synthetic and real data has been used extensively to validate models, but we further elaborate on the standard principle of comparison over both types of data by contrasting the predictive performance across types of synthetic data.  The hypothesis we make is that the relative performance of different models will be stable by the characteristic of a given type of data, as defined by the underlying ground truth for real data, or by the model that generates the synthetic data.  We explore this hypothesis in the domain of Educational Data Mining and the assessment of student skills, where a set of latent skills are mapped to question items and students skill mastery is inferred from item outcome results from test data. 


This chapter introduces and defines these concepts, as well as outlines the objectives and main scientific hypotheses of the proposed research. The final section presents the organization of the remainder of this research.


\section{Research Questions}
\paragraph{}The following questions are addressed in this thesis:

\begin{enumerate}
\item What is the relative performance of student skills assessment models over real and over synthetic data created using the same models?
\item Is the relative performance unique to each synthetic data type (data from the same ground truth model)?
\item Can the relative performance be used to define a method to reliably identify the ground truth behind the synthetic data?
\item How does the method compare with the standard practice of using the model with the best performance?  In particular, does the ground truth model identified better generalize over a space of parameter values?
\end{enumerate}


\section{General Objectives}

\paragraph{}The general objective of this thesis is to assess model fit on the bases of the goodness of fit measure with predictive performance analysis of real and synthetic data. Fundamentally it can be divided in three sub-objectives: The first objective is to obtain the predictive performance of student skills assessment models over a dataset using the same models. This will create a vector of performances in the performance space. The second one is to assess model fit using the relative performance vector of the synthetic and real data. The third objective is to test the uniqueness and sensitivity of the performance vector on the different data specific conditions such as sample size, nose, average success rate.

\section{Hypotheses}
\paragraph{}The research in this thesis tests the following hypotheses:
\paragraph{Hypothesis 1:} The relative performances of two datasets with the same underlying models have high correlation.
\paragraph{Hypothesis 2:} The best performer model in the predictive performance vector is not necessarily the ground truth.
\paragraph{Hypothesis 3:} Datasets with the same model parameters and data specific parameters create unique performance vector. \note{ or Relative performance of each synthetic data type is unique} \note{or the relative performance of different models will be stable by the characteristic of a given type of data}
\paragraph{Hypothesis 4:} Datasets with the same underlying  models but different data specific parameters can have different performance vectors.

\section{Main Contributions}

The main contribution of this thesis is assessing model fit using the relative predictive performance vector of synthetic and real data. This method can be applied to different fields of studies but in this research we focused on student test result and few skills assessment models that have emerged mostly in EDM and ITS. The predictive performance of each model is assessed by designing an experiment which learns the model parameters and observes a set of items for a student to predict the rest of items test results of that student. The mean predictive accuracy will be the predictive performance measure. Previous researches compared their predictive performance  on a pairwise basis, but few studies have taken a comprehensive approach to compare them on a common basis. In this research we used seven skills assessment models to obtain the predictive performance vector using the same models. This vector can be considered from two perspectives: The first one is a kind of ``signature'' for a specific data which considers the vector in a two dimensional space which are performances over skills assessment models. The second perspective is a performance vector in the performance space where we have the same number of dimensions as the number of models in the performance vector. They are sharing the same concepts but different presentations.

The next step is to use this performance vector to assess model fit for a real dataset. The standard practice is to pick the ``best performer'' as the ground truth model. The actual best fitting model may have been overlooked due to an unfortunate choice of the algorithm's parameters.  Therefore, the best performer may not be the model that is most representative of the ground truth, but instead it may be the result of contextual factors that make this model outperform the ground truth one. We investigate the question of assessing different model fits using synthetic data by defining a vector space based on model performances, and use a nearest neighbor approach on the bases of correlation to identify the ground truth model. Comparing the performance of synthetic dataset with an specific underlying model and the performance of a real dataset with the same underlying model should show a high correlation. 

Still the question of sensitivity of contextual factors should be considered in the comparison of the performance vectors. The other contribution is to test the stability of the ``signature'' of synthetic datasets with different data specific parameters (such as sample size, average success rate and ect.) but the same underlying models. Based on the results of this experiment, assessing model fit should be in a condition where both synthetic and real data have the same data specific parameters.

In this study, we used a methodology to assess the model fit. The methodology requires a set of synthetic datasets with each skills assessment models that uses the contextual parameters of real data and a cross validation process to get the predictive performance from both synthetic and real data using the same models. The nearest neighbor of the synthetic datasets performance vectors to the real data performance vector in the performance space identifies the ground truth. This contribution is discussed in detail in Chapter \ref{sec:SIGNATURE}.

Considering the proposed (``signature'')method  and ``best performer'' approach as two classification approaches to assess model fit, results show that the ``signature'' approach is more accurate than the ``best performer'' , but only for some ground truth models. \note{not sure to put this or not? }


\section{Publications}
\begin{enumerate}

\item \textbf{B. Beheshti}, M.C. Desmarais, “Assessing Model Fit With Synthetic vs. Real Data" , Journal Submitted to \textbf{Journal of Educational Data Mining}.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Goodness of Fit of Skills Assessment Approaches: Insights from Patterns of Real vs. Synthetic Data Sets", Short Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain, pp: 368-371.

\item \textbf{B. Beheshti}, M.C. Desmarais, R. Naceur, “Methods to Find the Number of Latent Skills”, short paper in \textbf{International Educational Data Mining 2012} July 2012, Crete,Greece. , pp: 81-86.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Improving matrix factorization techniques of student test data with partial order constraints", Doctoral consortium in \textbf{User Modeling, Adaptation, and Personalization 2012} Aug 2012, Montreal,Canada. , pp: 346-350.

\item M.C. Desmarais, \textbf{B. Beheshti}, P. Xu, “The refinement of a q-matrix: assessing methods to validate tasks to skills mapping", Short paper in \textbf{International Educational Data Mining 2014} June 2014, London,United Kingdom., pp: 308-3011.

\item M.C. Desmarais, \textbf{B. Beheshti}, R. Naceur, “Item to skills mapping: deriving a conjunctive q-matrix from data”, short paper in \textbf{Intelligent Tutoring Systems 2012} July 2012, Crete,Greece. , pp: 454-463.

\item M.C. Desmarais, P. Xu, \textbf{B. Beheshti}, “Combining techniques to refine item to skills Q-matrices with a partition tree", Full Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain., pp: 29-36.

\item M.C. Desmarais, R. Naceur, \textbf{B. Beheshti}, “Linear models of student skills for static data", Workshop in \textbf{User Modeling, Adaptation, and Personalization 2012} July 2012, Montreal, Canada.
\end{enumerate}


\section{Organization Of the Thesis}
\paragraph{} We review some of related literature on fundamental concepts in educational data mining and some machine learning techniques that have been used in our experiments in Chapter \ref{sec:RevLitt}. Chapter \ref{sec:RelatedWorks} discusses about some recent related works about model selection. As a complementary part of the main contribution we explain synthetic data generation approaches in chapter \ref{sec:Syn}. The main contribution of the research is explained in details in Chapter \ref{sec:SIGNATURE} as summarized above. Finally, we conclude and outline future work in Chapter \ref{sec:Conclusion}. 


