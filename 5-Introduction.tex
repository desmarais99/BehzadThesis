\selectlanguage{english}
\Chapter{INTRODUCTION}\label{sec:Introduction} 

\section{Problem Definition and Challenges}
\paragraph{}
\selectlanguage{english} 

In Educational Data Mining, or in Data Science in general, analysts that wish to build a classification or regression model over new and unknown data are faced with a very wide span of choices.  Machine learning techniques nowadays offer the possibility to learn and train a large and an ever growing variety of models from data.  Learning techniques such as the E-M algorithm and MCMC methods have contributed to this expansion of models we can learn from data.  They allow model parameters estimation that would otherwise represent an intractable problem using standard analytical or optimization techniques.

Along with this increased display of models that can be defined and trained from data, comes the question of deciding which are the most representative of the underlying ground truth.  This question is of interest from two perspectives.  One is the theoretical and explanatory value of uncovering a model that accounts for observed data.  The other perspective is the assumption that the ``true'' underlying model will better generalize to samples other than the training data.  This assumption is commonly supported in physics where some models have a window in the parameter space where they correctly account for observations, and break down outside that window; Newtownian and modern physics are prototypical examples supporting this assumption.  

In the machine learning field, the case for the support of the assumption that the closer to the ground truth a model is, the better it will generalize outside the parameter space, is not as evident as it can be in physics. But we do find analogous examples such as the Naïve Bayes classifier under a 0-1 loss function tend to perform very well in spite of the unrealistic assumption of the naïve independence assumption at the root of the approach's name \citep{domingos1997}.

Given that in machine learning, we are often more interested in the predictive power of models than we are in their theoretical and explanatory value, the standard practice is to choose the model with the best predictive performance.  And without good theoretical understanding of the domain, we simply hope that it will generalize outside the space covered by our training sample.  

This thesis aims to provide a means to assess the fit of the model to the underlying ground truth using a methodology based on synthetic data, and to verify if the approach is better able to identify a model that will generalize outside the parameter space of the training sample.  The study is circumscribed to the domain of Educational Data Mining where we find numerous competing models of student skills mastery.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model selection and goodness of fit}

Model selection is the task of selecting a statistical model for a given data from a set of candidate models.  Selection is most often based on a model's ``goodness of fit''.

On the hand the term ``goodness of fit''  for a statistical model describes how well it fits a set of observation. The distance between observed values and the predicted values under the model can be a measure of goodness of fit. The goodness of fit is usually determined using likelihood ratio. There exist different approaches to assess model fit based on the measure of goodness of fit. The consensus is that the model with the best predictive performance is the most likely to be the closest to the ground Truth. Then there are the issues of how sensitive is the model to sample size, noise, and biases that also need to be addressed before we can trust that this model is the best candidate. It can take numerous studies before a true consensus emerges as to which model is the best candidate for a given type of data.

Another approach to assess which model is closest to the ground truth is proposed in this thesis. It relies on the analysis of model predictive performances over real data and synthetic data. Using synthetic data allows us to validate the sensitivity of the model selection approach to data specific parameters such as sample size and noise. Comparing performance over synthetic and real data has been used extensively to validate models, but we further elaborate on the standard principle of comparison over both types of data by contrasting the predictive performance across types of synthetic data.  The hypothesis we make is that the relative performance of different models will be stable by the characteristic of a given type of data, as defined by the underlying ground truth for real data, or by the model that generates the synthetic data.  We explore this hypothesis in the domain of Educational Data Mining and the assessment of student skills, where latent skills are mapped to question items and students skill mastery is inferred from item outcome results from test data. 


This chapter introduces and defines these concepts, as well as outlines the objectives and main scientific hypotheses of the proposed research. The final section presents the organization of the remainder of this research.

\section{Thesis vocabulary}
\label{Vocabl}

In this section we introduce a vocabulary that is related to the general objective of this thesis and used in all chapters:

\begin{itemize}
\item \textbf{Student model}: "In general terms, student modeling involves the construction of a qualitative representation that accounts for student behavior in terms of existing background knowledge about a domain and about students learning the domain. Such a representation, called a student model" \citep{sison1998student}. Student skills assessment models are essentially constructed to assess student's skills or estimate potential skills required for problems. Since our experiments are in the domain of educational data mining, by the term ``Model'' we mean ``student model''. 
\item \textbf{Dataset (Real/Synthetic)}: Dataset in this context represents student test outcome which is a matrix that shows the result of a test given by students. A test is simply a set of few questions, problems or items that can have a success of a failure result in the dataset. Datasets can be ``real'' or ``synthetic''. A ``Real'' dataset is the result of an actual test given by individuals in an e-learning environment or even a classroom. The term ``Synthetic'' means that a simulation is involved to generate an artificial student test outcome. The simulation is designed based on a model that takes a set of predefined parameters to generate student test outcome. This set has two types of parameters: Model specific parameters and Data specific parameters.
\item \textbf{Model specific parameters}: These parameters are specifically defined and learnt based on model's type. Complex models contain more parameters. Some models may share some parameters but some models have no parameters in common.
\item \textbf{Data specific parameters}: These parameters are common between all datasets which are also known as ``Contextual parameters'' such as average success rate, sample size and number of items in a dataset.
\item \textbf{Ground truth}: This term is originally coined by Geographical/earth science where if a location method such as GPS estimates a location coordinates of a spot on earth, then the actual location on earth would be the ``Ground truth''. This term has been adopted in other fields of study. In this context ``ground truth of a dataset'' means the actual model that best describes the dataset within its parameters.  Note that for the skills models studied here, the ground truth is always unknown, unless we use synthetic data.
\item \textbf{Performance of a model}: The accuracy of a model to predict student response outcomes over a dataset (using cross-validation) is called performance of a model.  Different models have different performances over a dataset. Assessing such a performance requires designing an experiment to learn the model's parameters and predict a proportion of the dataset that has not been involved in the learning phase.
\item \textbf{Performance vector}: Each model has a performance over a dataset. Consider a set of models, $\mathcal{M}$, and a vector~$\mathbf{p}$ of length~$|\mathcal{M}|$ that contains the performance of each model over a given data set.  This vector represents a point in the performance space, and it is defined as the \textit{performance vector} of that data set.

\item \textbf{Performance Signature}: The \textit{performance vector} can be considered from two perspectives: The first perspective is a \textit{performance vector} in the performance space where we have the same number of dimensions as the number of candidate models in the \textit{performance vector}. The second one is a kind of \textit{performance signature} for a specific data that considers the vector in a single dimensional space which is essentially a line representing performances of the candidate models.  They are sharing the same concepts but different presentations. We will refer to \textit{performance signature} for the purpose of easier visualization in a single dimensional space.

\item \textbf{Performance Prototype}: We use the term \textit{performance prototype} to designate the \textit{performance vector} associated with the synthetic data of a model class.  Note that there are different ways to produce the synthetic data of a model and we return to this question later.

\item \textbf{Target Performance Vector}: This term is used to refer to the \textit{performance vector} of the real data set to classify.


\end{itemize}

\section{Research Questions}
\paragraph{}The following questions are addressed in this thesis:

\begin{enumerate}
\item What is the \textit{performance vector} of student skills assessment models over real and over synthetic data created using the same models?
\item Is the \textit{performance vector} unique to each synthetic data type (data from the same ground truth model)?
\item Can the \textit{performance vector} be used to define a method to reliably identify the ground truth behind the synthetic data?
\item How does the method compare with the standard practice of using the model with the best performance? % In particular, does the ground truth model identified better generalize over a space of parameter values?
\end{enumerate}


\section{General Objectives}

\paragraph{}The general objective of this thesis is to assess the goodness of fit on the basis of the what we will refer to as performance signatures. It can be divided in three sub-objectives: The first objective is to obtain the performance signatures of skills assessment models over a synthetic datasets generated with these very same models. This will create a vector of performances in the performance space. The second one is to assess model fit using the \textit{performance vector} of the synthetic and real data. The third objective is to test the uniqueness and sensitivity of the \textit{performance vectors} on the different data specific conditions such as sample size, nose, average success rate.

\section{Hypotheses}
\paragraph{}The research in this thesis tests the following hypotheses:
\paragraph{Hypothesis 1:} The \textit{performance vectors} of two datasets with the same ground truth have a high level of correlation across different data specific parameters. 
\paragraph{Hypothesis 2:} The best performer model is not necessarily the ground truth model.
\paragraph{Hypothesis 3:} Datasets with the same model parameters and data specific parameters create unique \textit{performance vector}. 
%The following hypothesis can also be considered :
%\paragraph{Hypothesis 4:} If the ground truth model is different than the best performer model for a given data set, the expected value of the performance of the ground truth model over the space of data and model parameters is greater
%it's a tautology that the best performer is always better in terms of expected performance comparing to any other method then it should be a false hypothesis.
% the contribution for this hypothesis would be : giving different weight to different parts of the space to justify the performance unless we do that to emphasize that all the space is equally important-> can be tested with synthetic dataset

\section{Main Contributions}

The main contribution of this thesis is assessing model fit of a data set by comparing its \textit{performance vector} to the \textit{performance prototype} of synthetic datasets generated with the same data specific parameters of the given dataset but different skills assessment models. This method can be applied to different fields of studies but in this research we focus on student test result and on a few skills assessment models that have emerged mostly in EDM and ITS. The predictive performance of each model is assessed by designing an experiment, which learns the model parameters and observes a set of items for a student to predict the rest of items test results of that student. The mean predictive accuracy will be the predictive performance measure. Previous researches compared their predictive performance on a pairwise basis, but few studies have taken a comprehensive approach to compare them on a common basis. In this research we used seven skills assessment models to obtain the predictive \textit{performance vector} using the same models. 

The next step is to use this \textit{performance vector} to assess model fit for a real dataset. The standard practice is to pick the ``best performer'' as the ground truth model. The actual best fitting model may have been overlooked due to an unfortunate estimate of the algorithm's parameters.  Therefore, the best performer may not be the model that is most representative of the ground truth, but instead it may be the result of contextual factors that make this model outperform the ground truth one. We investigate the question of assessing different model fits using synthetic data by defining a vector space based on model performances, and use a nearest neighbor approach on the bases of correlation to identify the ground truth model. Comparing the performance of synthetic dataset with a specific underlying model and the performance of a real dataset with the same underlying model should show a high correlation. 

Still the question of sensitivity of the \textit{performance signature} to contextual factors should be considered in the comparison of the \textit{performance vectors}. The other contribution is to test the stability of the ``performance signature'' of synthetic datasets over different data specific parameters (such as sample size, average success rate, etc.) generated with the same underlying model. 

%adding generality of the approach across the whole space

\section{Publications}

Along the course of the doctorate studies, I contributed to a number of publications, some of which are directly related to this thesis, and some of which are peripheral or are preliminary studies that led to the thesis.

\begin{enumerate}

\item \textbf{B. Beheshti}, M.C. Desmarais, “Assessing Model Fit With Synthetic vs. Real Data" , Journal Submitted to \textbf{Journal of Educational Data Mining}.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Goodness of Fit of Skills Assessment Approaches: Insights from Patterns of Real vs. Synthetic Data Sets", Short Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain, pp: 368-371.

\item \textbf{B. Beheshti}, M.C. Desmarais, R. Naceur, “Methods to Find the Number of Latent Skills”, short paper in \textbf{International Educational Data Mining 2012} July 2012, Crete, Greece. , pp: 81-86.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Improving matrix factorization techniques of student test data with partial order constraints", Doctoral consortium in \textbf{User Modeling, Adaptation, and Personalization 2012} Aug 2012, Montreal, Canada. , pp: 346-350.

\item M.C. Desmarais, \textbf{B. Beheshti}, P. Xu, “The refinement of a q-matrix: assessing methods to validate tasks to skills mapping", Short paper in \textbf{International Educational Data Mining 2014} June 2014, London, United Kingdom., pp: 308-3011.

\item M.C. Desmarais, \textbf{B. Beheshti}, R. Naceur, “Item to skills mapping: deriving a conjunctive q-matrix from data”, short paper in \textbf{Intelligent Tutoring Systems 2012} July 2012, Crete, Greece. , pp: 454-463.

\item M.C. Desmarais, P. Xu, \textbf{B. Beheshti}, “Combining techniques to refine item to skills Q-matrices with a partition tree", Full Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain., pp: 29-36.

\item M.C. Desmarais, R. Naceur, \textbf{B. Beheshti}, “Linear models of student skills for static data", Workshop in \textbf{User Modeling, Adaptation, and Personalization 2012} July 2012, Montreal, Canada.
\end{enumerate}


\section{Organization of the Thesis}
\paragraph{} We review the related literature on fundamental concepts in Educational Data Mining and some machine learning techniques that have been used in our experiments in Chapter \ref{sec:RevLitt}. Chapter \ref{sec:RevLitt} also discusses recent work about model selection. The main contribution of the research starts from chapter \ref{sec:Approach} where we explain the proposed approach in details. As a complementary part of the proposed approach we explain synthetic data generation approaches in chapter \ref{sec:Syn}. The experimental results of the main contribution are explained in details in Chapter \ref{sec:SIGNATURE}. Finally, we conclude and outline future work in Chapter \ref{sec:Conclusion}. 


