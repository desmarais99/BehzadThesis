\selectlanguage{english}
\Chapter{INTRODUCTION}\label{sec:Introduction} 

\section{Problem Definition and Challenges}
\paragraph{}
\selectlanguage{english} 

Data analysts that wish to build a classification or regression model over new and unknown data are faced with a very wide span of choices.  Machine learning techniques nowadays offer the possibility to learn and train a large and an ever growing variety of models from data.  Learning techniques such as the E-M algorithm and MCMC methods have contributed to this expansion of models we can learn from data.  They allow model parameters estimation that would otherwise represent an intractable problem using standard analytical or optimization techniques.

Along with this increased display of models that can be defined and trained from data, comes the question of deciding which are the most representative of the underlying ground truth.  This question is of interest from two prespective.  One is the theoretical and explanatory value of uncovering a model that accounts for observed data.  The other perspective is the assumption that the ``true'' underlying model will better generalize to samples other than the training data.  This assumption is commonly supported in physics where some models have a window in the parameter space where they correctly account for observations, and break down outside that window; Newtownian and modern physics are prototypical examples supporting this assumption.  

In the machine learning field, the case for the support of the assumption that the closer to the ground truth a model is, the better it will generalize outside the parameter space, is not as evident as it can be in physics. But we do find analoguous examples such as the Naïve Bayes classifier nder a 0-1 loss function tend to perform very well in spite of the unrealistic assumption of the naïve independence assumption at the root of the approach's name\footnote{Domingos, P. & Pazzani, M. On the optimality of the simple Bayesian classifier under zero-one loss Machine Learning, 1997, 29, 103-130}.

Given that in machine learning, we are often more interested in the predictive power of models than we are in their theoretical and explanatory value, the standard practice is to choose the model with the best predictive performance.  And without good theoretical understanding of the domain, we simply hope that it will generalize outside the space covered by our training sample.  

This thesis aims to provide a means to assess the fit of the model to the underlying ground truth using a methodology based on synthetic data, and to verify if the approach is better able to identify a model that will generalize outside the parameter space of the training sample.  The study is circumscribed to the domain of Educational Data Mining where we find numerous competing models of student skills mastery.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model fitting and model selection}

Model fitting is the task of selecting a statistical model for a given data from a set of candidate models. Both data and the set of candidate models can involve the design of experiments that the data collected is well-suited to the problem of model selection or the candidate models best describe the data among a large number of models.\note{??} Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice. \citet{konishi2008information}\note{notice in the code here the use of \texttt{citet} which is a different way of citing} state, "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling".\note{relevant?}

A statistical experiment should be designed to select the bets of candidate models. What is meant by best is controversial. It can be defined from two perspectives: the best representative which is the goodness of fit in model selection and the simplicity of the model. The goodness of fit is usually determined using likelihood ratio. The complexity of the model can be presumed as the number of paremeters it contains. The more complex the model is, the better it adapts its shape to fit the data when some parameters may not represent anything useful. A model is more complex when it has more number of parameters. \note{Almost every sentence is touching a different issue.}


Simulated data has been increasingly playing an important role in EDM \citep{JEDM:baker2009}. Even in natural science domain, it is usually used as a way of evaluating its underlying model. As an example \citep{jafarpour2015quantifying} used simulated data for disease outbreak detection where simulated data is generated from a hypothesized model of a phenomena and if there exists similarities between simulated data and real data observed in the nature, it serves as an evidence for the accuracy of the model and it may be possible to claim that the model provides an authentic explanation of the system. \note{not clear where the argument is leading to}

Yet another approach to assess which model is closest to the ground truth is to combine predictive performance analysis of real data with synthetic data.  Comparing performance over synthetic and real data has been used extensively to validate models, but we further elaborate on the standard principle of comparison over both types of data by contrasting the predictive performance across types of synthetic data.  The hypothesis we make is that the relative performance of different models will be stable by the characteristic of a given type of data, as defined by the underlying ground truth for real data, or by the model that generates the synthetic data.  We explore this hypothesis in the domain of Educational Data Mining and the assessment of student skills, where a set of latent skills are mapped to question items and students skill mastery is inferred from item outcome results from test data. We will discuss the consistency of this approach over different aspects of the test result dataset which can affect the predictive performance of a model. According to this sensitivity we can find an appropriate measure to set a degree of similarity between the synthetic data and the ground truth.

In this research we want to show the similarity between real learners and simulated learners\note{Really?}. Testing learning systems (for example Intelligent tutoring systems) with real students are costly and time consuming. if simulated data could represent the behaviour of the real learners given a model, then the implications are that we can use synthetic data to test ITS without spending too much time to test a system on real students. The problem of model fitting is a general problem that can be applied to other fields of study such as natural science or even marketing strategy where consumer models are used for time and cost efficiency purposes.



This chapter introduces and defines these concepts, as well as outlines the objectives and main scientific hypotheses of the proposed research. The final section presents the organization of the remainder of this proposal.


\section{Research Questions}
\paragraph{}The following questions are addressed in this thesis:

\note{Changing the research questions because, except for the first one, they have no relation with the introduction so far. They have to be in line with the introduction and the main topic.}
\begin{enumerate}
\item What is the relative performance of student skills assessment models over real and over synthetic data created using the same models?
\item Is the relative performance unique to each synthetic data type (data from the same ground truth model)?
\item Can the relative performance be used to define a method to reliably identify the ground truth behind the synthetic data?
\item How does the method compare with the standard practice of using the model with the best performance?  In particular, does the ground thruth model identified better generalize over a space of parameter values?
\end{enumerate}


\section{General Objectives}
\paragraph{}This research has two main general objectives. The first objective is to assess a model fit with predictive performance of different data mining techniques over synthetic vs. real data. Defining a measure of similarity for this assessment is also the \annote{goal of this objective}{always re-read yourself!}.

\paragraph{}The second research objective is to improve the quality of deriving a Q-matrix as well as automatizing the process according to the data mining techniques. This improvement can be achieved by refining a pre-defined Q-matrix where minor errors exist in mapping or by defining approaches to deal with different types of Q-matrices. Also some parts of this process can be automatized such as prediction for an optimum number of skills behind a set of questions.

\section{Hypotheses}
\paragraph{}The research in this thesis tests the following hypotheses:
\paragraph{Hypothesis 1:} The predictive performances of different skills assessment techniques of two datasets with the same model are \annote{much}{avoid superlatives} similar than those with different models. 
\paragraph{Hypothesis 2:} The predictive performance of factorization techniques reaches its maximum value once an optimum number of skills applied in the process.\note{This comes out of the blue}
\paragraph{Hypothesis 3:}Matrix factorization techniques can derive Q-matrices with multi-skilled items as well as single skill items. \note{nothing about the main contribution?}

\section{Main Contributions}
\paragraph{}There are two contributions in this thesis. The first one is the main contribution and the second one is a sub-contribution of the first one:
\begin{enumerate}
\item The main contribution of this thesis is assessing model fit with comparison between the predictive performance of synthetic and real data. This method can be applied to different fields of studies but in this research we focused on student test result and few skills assessment models that have emerged mostly in EDM and ITS. Previous researches compared their predictive performance  on a pairwise basis, but few studies have taken a comprehensive approach to compare them on a common basis. Comparing the performance of synthetic dataset with an specific underlying model and the performance of a real dataset with the same underlying model should show a close correlation. 

In this study, we used a methodology to complete this comparison. The methodology requires a set of synthetic datasets with different skills assessment models and a cross validation process to get the predictive performance from both synthetic and real data. Once the Predictive performances of each dataset have achieved, they can create a kind of signature for each specific data. If this signature is unique, it might reveal the latent structure of the skills. There are plenty of factors to be considered to check the uniqueness of this signature for each model which can be variables for generation of data such as sample size, number of items, number of skills and average success rate. In our experiment, we will consider this variables but the details will be presented in the next chapters. Once the signature of both real and synthetic data is created we can calculate the correlation of these values to find a measure of similarity. Since we know the underlying model of the synthetic data, this correlation can give us a hint for model fitting. This contribution is discussed in detail in Chapter \ref{sec:SIGNATURE}.

\item \note{There will need to be an introduction to this topic.} The next contribution is about validating the Q-matrix. Most of the linear skills assessment data mining techniques require to deal with Q-matrices and it is necessary to get the perfect one. There has been many efforts on improvement of Q-matrix \citep{Barnes2005,Desmarais2011b,Winters07}. In this research we will represent few contributions on improving the derived Q-matrix. The matrix factorization approach to inferring the Q-matrix from data has been explored by a few researchers \citep{Barnes2005,Winters07}, but for Q-matrices that involved only a single skill per item.They investigated the \ac{NMF} \citep{Berry2007,Desmarais2011b} technique and showed that it works very well for simulated data, but the technique's performance with real data was degraded. We extend a technique based on Non-negative Matrix Factorization, that was previously shown successful for single skill items, to construct a conjunctive item to skills mapping from test data with multiple skills per item. 

The other work is to find an optimum number of skills for a set of items in Q-matrix. Because Multiple skills may be involved at various degree of importance, and skills may overlap and correlate. we investigate two techniques to determine this number. The \ac{SVD} is a known technique to find latent factors where the singular values represent the latent skills. The other approach is wrapper-based where we examine the prediction accuracy of linear models with different number of skills and the one that yields the best prediction accuracy through cross validation is considered the most appropriate.

Finally we'll introduce few approaches to refine a pre-defined Q-matrix. We compare three data driven techniques for the validation of skills-to-tasks mappings.  All methods start from a given mapping, typically obtained from domain experts, and use optimization techniques to suggest a refined version of the skills-to-task mapping.  To validate the different techniques, we inject perturbations in the Q-matrix and verify whether the original Q-matrix can be recovered. Tests are run over both simulated and real data. This contribution is presented in Chapter \ref{sec:RevLitt}.

\paragraph{}This research mainly focuses on real data model fitting; 

\end{enumerate}

\section{Publications}
\begin{enumerate}

\item \textbf{B. Beheshti}, M.C. Desmarais, “Assessing Model Fit With Synthetic vs. Real Data" , Journal Submitted to \textbf{Journal of Educational Data Mining}.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Performance of prevailing approaches to skills assessment: Insights from patterns of real vs. synthetic data sets", Short Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain.\note{page numbers for all pub.}

\item \textbf{B. Beheshti}, M.C. Desmarais, “Predictive performance of prevailing approaches to skills assessment techniques: Insights from real vs. synthetic data sets", Poster in \textbf{International Educational Data Mining 2014} June 2014, London,United Kingdom.\note{I think we should skip this one because it has the same title.}

\item \textbf{B. Beheshti}, M.C. Desmarais, R. Naceur, “Methods to Find the Number of Latent Skills”, short paper in \textbf{International Educational Data Mining 2012} July 2012, Crete,Greece.

\item \textbf{B. Beheshti}, M.C. Desmarais, “Improving matrix factorization techniques of student test data with partial order constraints", Doctoral consortium in \textbf{User Modeling, Adaptation, and Personalization 2012} Aug 2012, Montreal,Canada.

\item M.C. Desmarais, \textbf{B. Beheshti}, P. Xu, “The refinement of a q-matrix: assessing methods to validate tasks to skills mapping", Short paper in \textbf{International Educational Data Mining 2014} June 2014, London,United Kingdom.

\item M.C. Desmarais, \textbf{B. Beheshti}, R. Naceur, “Item to skills mapping: deriving a conjunctive q-matrix from data”, short paper in \textbf{Intelligent Tutoring Systems 2012} July 2012, Crete,Greece.

\item M.C. Desmarais, P. Xu, \textbf{B. Beheshti}, “A partition tree approach to combine techniques to refine item to skills Q-matrices", Full Paper  in \textbf{International Educational Data Mining 2015} June 2015, Madrid, Spain.

\item M.C. Desmarais, R. Naceur, \textbf{B. Beheshti}, “Linear models of student skills for static data", Workshop in \textbf{User Modeling, Adaptation, and Personalization 2012} Aug 2012w, Montreal, Canada.
\end{enumerate}


\section{Organization Of the Thesis}
\paragraph{} We review some of related literature on fundamental concepts in educational data mining and some machine learning techniques that have been used in our experiments in Chapter \ref{sec:RevLitt}. Chapter \ref{sec:RelatedWorks} discusses about some recent related works about model fitting. As a complementary part of the main contribution we explain synthetic data generation approaches in chapter \ref{sec:Syn}. The main contribution of the research is explained in details in Chapter \ref{sec:SIGNATURE} as summarized above. Finally, we conclude and outline future work in Chapter \ref{sec:Conclusion}. 


